@article{SYRIANI2024101287,
title = {Screening articles for systematic reviews with ChatGPT},
journal = {Journal of Computer Languages},
volume = {80},
pages = {101287},
year = {2024},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2024.101287},
url = {https://www.sciencedirect.com/science/article/pii/S2590118424000303},
author = {Eugene Syriani and Istvan David and Gauransh Kumar},
keywords = {Generative AI, GPT, Empirical research, Large language model, Literature review, Mapping study, Screening},
abstract = {Systematic reviews (SRs) provide valuable evidence for guiding new research directions. However, the manual effort involved in selecting articles for inclusion in an SR is error-prone and time-consuming. While screening articles has traditionally been considered challenging to automate, the advent of large language models offers new possibilities. In this paper, we discuss the effect of using ChatGPT on the SR process. In particular, we investigate the effectiveness of different prompt strategies for automating the article screening process using five real SR datasets. Our results show that ChatGPT can reach up to 82% accuracy. The best performing prompts specify exclusion criteria and avoid negative shots. However, prompts should be adapted to different corpus characteristics.}
}
@article{GAROUSI2020110570,
title = {Software-testing education: A systematic literature mapping},
journal = {Journal of Systems and Software},
volume = {165},
pages = {110570},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110570},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220300510},
author = {Vahid Garousi and Austen Rainer and Per Lauvås and Andrea Arcuri},
keywords = {Software testing, Software-testing education, Software-engineering education, Education research, Systematic literature review, Systematic literature mapping},
abstract = {Context
With the rising complexity and scale of software systems, there is an ever-increasing demand for sophisticated and cost-effective software testing. To meet such a demand, there is a need for a highly-skilled software testing work-force (test engineers) in the industry. To address that need, many university educators worldwide have included software-testing education in their software engineering (SE) or computer science (CS) programs. Many papers have been published in the last three decades (as early as 1992) to share experience from such undertakings.
Objective
Our objective in this paper is to summarize the body of experience and knowledge in the area of software-testing education to benefit the readers (both educators and researchers) in designing and delivering software testing courses in university settings, and to also conduct further education research in this area.
Method
To address the above need, we conducted a systematic literature mapping (SLM) to synthesize what the community of educators have published on this topic. After compiling a candidate pool of 307 papers, and applying a set of inclusion/exclusion criteria, our final pool included 204 papers published between 1992 and 2019.
Results
The topic of software-testing education is becoming more active, as we can see by the increasing number of papers. Many pedagogical approaches (how to best teach testing), course-ware, and specific tools for testing education have been proposed. Many challenges in testing education and insights on how to overcome those challenges have been proposed.
Conclusion
This paper provides educators and researchers with a classification of existing studies within software-testing education. We further synthesize challenges and insights reported when teaching software testing. The paper also provides a reference (“index”) to the vast body of knowledge and experience on teaching software testing. Our mapping study aims to help educators and researchers to identify the best practices in this area to effectively plan and deliver their software testing courses, or to conduct further education-research in this important area.}
}
@article{MAGABALEH2024111859,
title = {Systematic review of software engineering uses of multi-criteria decision-making methods: Trends, bibliographic analysis, challenges, recommendations, and future directions},
journal = {Applied Soft Computing},
volume = {163},
pages = {111859},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111859},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624006331},
author = {Aws A. Magabaleh and Lana L. Ghraibeh and Afnan Y. Audeh and A.S. Albahri and Muhammet Deveci and Jurgita Antucheviciene},
keywords = {MCDM, Software engineering, SDLC, Software development life cycle, Systematic literature review (SLR)},
abstract = {Correctly adhering to the processes within the software development life cycle (SDLC), from analysis and design to coding and testing, is vital for ensuring the successful and efficient creation of high-quality software applications. These structured phases provide a systematic approach to software development, facilitating clear communication, reducing errors, and improving collaboration among development teams. For the proper and correct use of SDLC processes, it is essential for both software engineers and software decision makers to perform the correct and needed actions while performing each software process, and one method of facilitating that is multicriteria decision making (MCDM). This study aims to provide a systematic review of the use of MCDM within the field of software engineering (SE), encompassing methodologies such as fuzzy MCDM, AHP, TOPSIS, DEMATEL, and other methods, with a deliberate focus on software engineering development processes. To ensure the high quality of this review, a methodical and structured literature search process was performed with strict selection criteria, resulting in the identification of 32 contributions on the applications of MCDM in SE from various databases, including Scopus, ScienceDirect, IEEE Xplore digital library (IEEE), and Web of Science (WOS). The selected papers were taxonomized into seven main categories, with some divided into subcategories. This paper presents a systematic and comprehensive analysis of the aforementioned studies, investigating the challenges, motivations, and recommendations found within each, thereby paving the way for potential future research. Bibliometric analysis is also provided to show concise quantitative analysis of related bibliographic information, which draws several key insights into publication trends. Finally, a critical analysis of the current literature and existing research is presented, while also addressing relevant research gaps.}
}
@article{GAROUSI2019101,
title = {Guidelines for including grey literature and conducting multivocal literature reviews in software engineering},
journal = {Information and Software Technology},
volume = {106},
pages = {101-121},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301939},
author = {Vahid Garousi and Michael Felderer and Mika V. Mäntylä},
keywords = {Multivocal literature review, Grey literature, Guidelines, Systematic literature review, Systematic mapping study, Literature study, Evidence-based software engineering},
abstract = {Context
A Multivocal Literature Review (MLR) is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts, videos and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). MLRs are useful for both researchers and practitioners since they provide summaries both the state-of-the art and –practice in a given area. MLRs are popular in other fields and have recently started to appear in software engineering (SE). As more MLR studies are conducted and reported, it is important to have a set of guidelines to ensure high quality of MLR processes and their results.
Objective
There are several guidelines to conduct SLR studies in SE. However, several phases of MLRs differ from those of traditional SLRs, for instance with respect to the search process and source quality assessment. Therefore, SLR guidelines are only partially useful for conducting MLR studies. Our goal in this paper is to present guidelines on how to conduct MLR studies in SE.
Method
To develop the MLR guidelines, we benefit from several inputs: (1) existing SLR guidelines in SE, (2), a literature survey of MLR guidelines and experience papers in other fields, and (3) our own experiences in conducting several MLRs in SE. We took the popular SLR guidelines of Kitchenham and Charters as the baseline and extended/adopted them to conduct MLR studies in SE. All derived guidelines are discussed in the context of an already-published MLR in SE as the running example.
Results
The resulting guidelines cover all phases of conducting and reporting MLRs in SE from the planning phase, over conducting the review to the final reporting of the review. In particular, we believe that incorporating and adopting a vast set of experience-based recommendations from MLR guidelines and experience papers in other fields have enabled us to propose a set of guidelines with solid foundations.
Conclusion
Having been developed on the basis of several types of experience and evidence, the provided MLR guidelines will support researchers to effectively and efficiently conduct new MLRs in any area of SE. The authors recommend the researchers to utilize these guidelines in their MLR studies and then share their lessons learned and experiences.}
}
@article{KHAN2018156,
title = {Empirical studies omit reporting necessary details: A systematic literature review of reporting quality in model based testing},
journal = {Computer Standards & Interfaces},
volume = {55},
pages = {156-170},
year = {2018},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0920548916302112},
author = {Muhammad Uzair khan and Sidra iftikhar and Muhammad Zohaib Iqbal and Salman Sherin},
keywords = {Empirical study, Reporting quality, Reporting guidelines, Model based testing},
abstract = {Context
Empirical studies are essential in evaluating the effectiveness of Model-based Testing (MBT) research and should be reported properly to ensure their replication and to highlight the strengths and limitations of the MBT techniques being evaluated. Researchers have proposed guidelines detailing what information should be reported when presenting empirical studies and what should be the structure of such primary studies. There is a need to evaluate the reporting quality of the empirical studies in MBT literature.
Objective
To evaluate the reporting quality of empirical studies in the model based testing domain; identifying where the reported studies fail to follow the proposed guidelines and finding frequently omitted details. As an auxiliary goal we aim to quantify the percentage of empirical studies conducted in industrial context.
Method
We evaluate the reporting quality and the execution contexts of MBT empirical studies reported in literature. For our study we consider the MBT papers published in top ten software engineering journals over the last eighteen years. We evaluate the published primary studies using the empirical study reporting guidelines.
Results
We found 87 empirical in MBT that met our selection criteria. Initial results showed that the existing guidelines were not only too strict (for example they demand presence of specific sections rather than simply having the details present in the paper), they also did not adequately cover MBT specific details. Therefore, we propose modified the guidelines for reporting empirical studies in MBT and re-evaluated the selected studies. Results show that while only a few empirical studies follow the exact structure proposed by the guidelines, approximately half the papers contain at least 50% of the required details. Most of the papers omit details related to process and analysis leading to presented results. We found a positive trend of improving reporting quality of empirical studies in MBT over the last Eighteen years. Another important finding from the review is that few reported studies were conducted in real industrial context.
Conclusions
Model based testing community needs to be more aware of the reporting guidelines and more effort should be spent on reporting the necessary details. Furthermore, we found that only few studies that are conducted in industrial context and hence more focus should be given to empirical case studies in real industry context. However, the reporting quality of research papers presenting empirical evaluations is gradually improving.}
}
@article{GIAMATTEI2025107599,
title = {Causal reasoning in Software Quality Assurance: A systematic review},
journal = {Information and Software Technology},
volume = {178},
pages = {107599},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107599},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924002040},
author = {Luca Giamattei and Antonio Guerriero and Roberto Pietrantuono and Stefano Russo},
keywords = {Causal reasoning, Causal discovery, Causal inference, Software quality},
abstract = {Context:
Software Quality Assurance (SQA) is a fundamental part of software engineering to ensure stakeholders that software products work as expected after release in operation. Machine Learning (ML) has proven to be able to boost SQA activities and contribute to the development of quality software systems. In this context, Causal Reasoning is gaining increasing interest as a methodology to go beyond a purely data-driven approach by exploiting the use of causality for more effective SQA strategies.
Objective:
Provide a broad and detailed overview of the use of causal reasoning for SQA activities, in order to support researchers to access this research field, identifying room for application, main challenges and research opportunities.
Methods:
A systematic review of the scientific literature on causal reasoning for SQA. The study has found, classified, and analyzed 86 articles, according to established guidelines for software engineering secondary studies.
Results:
Results highlight the primary areas within SQA where causal reasoning has been applied, the predominant methodologies used, and the level of maturity of the proposed solutions. Fault localization is the activity where causal reasoning is more exploited, especially in the web services/microservices domain, but other tasks like testing are rapidly gaining popularity. Both causal inference and causal discovery are exploited, with the Pearl’s graphical formulation of causality being preferred, likely due to its intuitiveness. Tools to favor their application are appearing at a fast pace — most of them after 2021.
Conclusions:
The findings show that causal reasoning is a valuable means for SQA tasks with respect to multiple quality attributes, especially during V&V, evolution and maintenance to ensure reliability, while it is not yet fully exploited for phases like requirements engineering and design. We give a picture of the current landscape, pointing out exciting possibilities for future research.}
}
@article{PACHECO20122171,
title = {A systematic literature review of stakeholder identification methods in requirements elicitation},
journal = {Journal of Systems and Software},
volume = {85},
number = {9},
pages = {2171-2181},
year = {2012},
note = {Selected papers from the 2011 Joint Working IEEE/IFIP Conference on Software Architecture (WICSA 2011)},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2012.04.075},
url = {https://www.sciencedirect.com/science/article/pii/S0164121212001288},
author = {Carla Pacheco and Ivan Garcia},
keywords = {Systematic review, Requirements engineering, Stakeholder identification, Requirements elicitation, Software engineering},
abstract = {This paper presents a systematic review of relevant published studies related to topics in Requirements Engineering, specifically, concerning stakeholder identification methods in requirements elicitation, dated from 1984 to 2011. Addressing four specific research questions, this systematic literature review shows the following evidence gathered from these studies: current status of stakeholder identification in software requirement elicitation, the best practices recommended for its performance, consequences of incorrect identification in requirements quality, and, aspects which need to be improved. Our findings suggest that the analyzed approaches still have serious limitations in terms of covering all aspects of stakeholder identification as an important part of requirements elicitation. However, through correctly identifying and understanding the stakeholders, it is possible to develop high quality software.}
}
@article{TRIPATHI2025112391,
title = {NoSQL database education: A review of models, tools and teaching methods},
journal = {Journal of Systems and Software},
volume = {226},
pages = {112391},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112391},
url = {https://www.sciencedirect.com/science/article/pii/S0164121225000597},
author = {Nirnaya Tripathi},
keywords = {NoSQL, Education, Teaching methods, Constructive alignment, Database, Systematic literature review, Software Engineering, Computer science},
abstract = {NoSQL databases are essential for managing modern data-intensive applications. While SQL education is a crucial part of the software engineering and computer science curriculum, it is insufficient in addressing the rise of big data and cloud infrastructures. Despite extensive research on SQL education, there is limited exploration of NoSQL education, particularly in teaching methods and data models. This study addresses this gap by conducting a systematic literature review on NoSQL database education, aiming to assess current research, teaching practices, models, tools, scalability, and security mechanisms while offering a framework for integrating NoSQL into academic curricula. Out of 386 articles, 28 were selected for detailed analysis, focusing on NoSQL teaching methods, models, and curriculum development. Findings revealed that document-oriented and graph databases, especially MongoDB, Cassandra, and Neo4j, are the most taught. The project-based learning approach was the most common teaching method. Challenges identified include adapting to technological advancements, addressing diverse student needs, and the shift to online learning. This review contributes valuable insights into NoSQL education and offers recommendations for improving teaching practices in software engineering curricula.}
}
@article{ALJEDAANI2025107825,
title = {An exploration study on developing blockchain systems–the practitioners' perspective},
journal = {Information and Software Technology},
volume = {186},
pages = {107825},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107825},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925001648},
author = {Bakheet Aljedaani and Aakash Ahmad and Mahdi Fehmideh and Arif Ali Khan and Jun Shen},
keywords = {Blockchain system, Empirical software engineering, Software development process},
abstract = {Context
Blockchain-based software (BBS) builds upon the foundational technologies of cryptocurrencies like Bitcoin, utilising decentralised, immutable ledgers, to support the development and operation of security-critical and transaction-intensive systems and services. In recent years, a number of research studies have investigated the strategic benefits and technical limitations of BBS that is central to the operations of a wide variety of systems ranging from cyber security, healthcare, education, and financial technologies. Despite an increasing interest both from academia and industry in BBS, there is a dearth of empirical evidence resulting in a lack of understanding about processes, methods, and techniques to enable a systematic development of this class of software systems.
Objectives
Existing research lacks a consolidated view, particularly empirically-driven guidelines based on published evidence and development practices. Therefore, our objective is to derive new or leverage existing development processes, patterns, and models to design, implement, and validate BBS systems.
Method
Tied to this knowledge gap, we conducted a two-phase research that unifies the findings of (i) a systematic literature review and (ii) practitioners’ survey to derive and validate the development process for BBS systems. First, we conducted a systematic literature review of 58 studies to derive a process comprising of 26 activities, to develop BBS systems. We than engaged 102 blockchain practitioners from, 35 countries across 6 continents to validate the BBS system development processes.
Results
Our results revealed a statistically significant difference (p-value < .001) in the importance ratings of 24 out of 26 BBS activities by our participants. The only two activities that were not statistically significant were incentive protocol design and granularity design. Our study also presented some of the activities that have been emphasised by our participants within the different development phases (i.e., Analysis Phase, Design Phase, Implementation Phase, Deployment Phase, and Execution and Maintenance Phase).
Conclusion
Our research is among the first to advance understanding on the aspect of development process for BBS and helps researchers and practitioners in their quests on challenges and recommendations associated with the development of BBS systems.}
}
@article{RICO2024107364,
title = {Experiences from conducting rapid reviews in collaboration with practitioners — Two industrial cases},
journal = {Information and Software Technology},
volume = {167},
pages = {107364},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107364},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923002197},
author = {Sergio Rico and Nauman Bin Ali and Emelie Engström and Martin Höst},
keywords = {Literature reviews, Systematic review, Rapid reviews, Research relevance, Industry-academia collaboration},
abstract = {Context:
Evidence-based software engineering (EBSE) aims to improve research utilization in practice. It relies on systematic methods to identify, appraise, and synthesize existing research findings to answer questions of interest for practice. However, the lack of practitioners’ involvement in these studies’ design, execution, and reporting indicates a lack of appreciation for the need for knowledge exchange between researchers and practitioners. The resultant systematic literature studies often lack relevance for practice.
Objective:
This paper explores the use of Rapid Reviews (RRs), in fostering knowledge exchange between academia and industry. Through the lens of two case studies, we delve into the practical application and experience of conducting RRs.
Methods:
We analyzed the conduct of two rapid reviews by two different groups of researchers and practitioners. We collected data through interviews, and the documents produced during the review (like review protocols, search results, and presentations). The interviews were analyzed using thematic analysis.
Results:
We report how the two groups of researchers and practitioners performed the rapid reviews. We observed some benefits, like promoting dialogue and paving the way for future collaborations. We also found that practitioners entrusted the researchers to develop and follow a rigorous approach and were more interested in the applicability of the findings in their context. The problems investigated in these two cases were relevant but not the most immediate ones. Therefore, rapidness was not a priority for the practitioners.
Conclusion:
The study illustrates that rapid reviews can support researcher-practitioner communication and industry-academia collaboration. Furthermore, the recommendations based on the experiences from the two cases complement the detailed guidelines researchers and practitioners may follow to increase interaction and knowledge exchange.}
}
@article{MORSCHHEUSER2018219,
title = {How to design gamification? A method for engineering gamified software},
journal = {Information and Software Technology},
volume = {95},
pages = {219-237},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S095058491730349X},
author = {Benedikt Morschheuser and Lobna Hassan and Karl Werder and Juho Hamari},
keywords = {Gamification, Software engineering, Design science research, Persuasive technology, Gameful design, Playfulness, Game design},
abstract = {Context
Since its inception around 2010, gamification has become one of the top technology and software trends. However, gamification has also been regarded as one of the most challenging areas of software engineering. Beyond traditional software design requirements, designing gamification requires the command of disciplines such as (motivational/behavioral) psychology, game design, and narratology, making the development of gamified software a challenge for traditional software developers. Gamification software inhabits a finely tuned niche of software engineering that seeks for both high functionality and engagement; beyond technical flawlessness, gamification has to motivate and affect users. Consequently, it has also been projected that most gamified software is doomed to fail.
Objective
This paper seeks to advance the understanding of designing gamification and to provide a comprehensive method for developing gamified software.
Method
We approach the research problem via a design science research approach; firstly, by synthesizing the current body of literature on gamification design methods and by interviewing 25 gamification experts, producing a comprehensive list of design principles for developing gamified software. Secondly, and more importantly, we develop a detailed method for engineering of gamified software based on the gathered knowledge and design principles. Finally, we conduct an evaluation of the artifacts via interviews of ten gamification experts and implementation of the engineering method in a gamification project.
Results
As results of the study, we present the method and key design principles for engineering gamified software. Based on the empirical and expert evaluation, the developed method was deemed as comprehensive, implementable, complete, and useful. We deliver a comprehensive overview of gamification guidelines and shed novel insights into the nature of gamification development and design discourse.
Conclusion
This paper takes first steps towards a comprehensive method for gamified software engineering.}
}
@article{UZUN201830,
title = {Model-driven architecture based testing: A systematic literature review},
journal = {Information and Software Technology},
volume = {102},
pages = {30-48},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918300880},
author = {Burak Uzun and Bedir Tekinerdogan},
keywords = {Model-based testing, Software architecture, Systematic review},
abstract = {Context
Model-driven architecture based testing (MDABT) adopts architectural models of a system under test and/or its environment to derive test artifacts. In the literature, different MDABT approaches have been provided together with the corresponding lessons results and lessons learned.
Objective
The overall objective of this paper is to identify the published concerns for applying MDABT, identify the proposed solutions, and describe the current research directions for MDABT.
Method
To this end we have provided a systematic literature review (SLR) that is conducted by a multi-phase study selection process using the published literature in major software engineering journals and conference proceedings.
Results
We reviewed 739 papers that are discovered using a well-planned review protocol, and 31 of them were assessed as primary studies related to our research questions. Based on the analysis of the data extraction process, we discuss the primary trends and approaches and present the identified obstacles.
Conclusion
This study shows that although a generic process the approaches different in various ways with different goals, modeling abstractions and results. Further, based on the synthesis process in the SLR we can state that the potential of MDABT has not been fully exploited yet.}
}
@article{BANO2015148,
title = {A systematic review on the relationship between user involvement and system success},
journal = {Information and Software Technology},
volume = {58},
pages = {148-169},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914001505},
author = {Muneera Bano and Didar Zowghi},
keywords = {User involvement, Software development, Systematic Literature Review},
abstract = {Context
For more than four decades it has been intuitively accepted that user involvement (UI) during system development lifecycle leads to system success. However when the researchers have evaluated the user involvement and system success (UI-SS) relationship empirically, the results were not always positive.
Objective
Our objective was to explore the UI-SS relationship by synthesizing the results of all the studies that have empirically investigated this complex phenomenon.
Method
We performed a Systematic Literature Review (SLR) following the steps provided in the guidelines of Evidence Based Software Engineering. From the resulting studies we extracted data to answer our 9 research questions related to the UI-SS relationship, identification of users, perspectives of UI, benefits, problems and challenges of UI, degree and level of UI, relevance of stages of software development lifecycle (SDLC) and the research method employed on the UI-SS relationship.
Results
Our systematic review resulted in selecting 87 empirical studies published during the period 1980–2012. Among 87 studies reviewed, 52 reported that UI positively contributes to system success, 12 suggested a negative contribution and 23 were uncertain. The UI-SS relationship is neither direct nor binary, and there are various confounding factors that play their role. The identification of users, their degree/level of involvement, stage of SDLC for UI, and choice of research method have been claimed to have impact on the UI-SS relationship. However, there is not sufficient empirical evidence available to support these claims.
Conclusion
Our results have revealed that UI does contribute positively to system success. But it is a double edged sword and if not managed carefully it may cause more problems than benefits. Based on the analysis of 87 studies, we were able to identify factors for effective management of UI alluding to the causes for inconsistency in the results of published literature.}
}
@article{CABRAL2023111542,
title = {Ensemble Effort Estimation: An updated and extended systematic literature review},
journal = {Journal of Systems and Software},
volume = {195},
pages = {111542},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111542},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002187},
author = {José Thiago H. de A. Cabral and Adriano L.I. Oliveira and Fabio Q.B. {da Silva}},
keywords = {Systematic literature review, Ensemble Effort Estimation, Software Engineering, Software effort estimation, Machine learning},
abstract = {Ensemble Effort Estimation (EEE) techniques combine several individual software estimation methods in order to address the weaknesses of individual methods for prediction tasks. A systematic review published in 2016 analyzed empirical studies on EEE techniques published between 2010 and (January) 2016. The research on EEE has continuously evolved over the past five years (2016–2020), generating new findings that should be aggregated to the existing body of evidence on the subject. The goal of this paper is to update the systematic review from 2016 with new findings from studies published between 2016 (full year) and 2020 (inclusive). To conduct our review update, we followed existing guidelines for updating systematic reviews in software engineering and other fields. We started with an appraisal of the background and methods of the 2016 review, which resulted in the updated review protocol used to conduct our study. We retrieved 3,682 papers using automatic searching techniques, from which we selected 30 papers for data extraction and analysis. Our findings reinforce the results of the previous review in that machine learning is still the technique most common to construct EEE and that the ensemble techniques have outperformed the individual models. We added new evidence showing that there is no clear superiority of an EEE model over the others. Also, we found that ensemble dynamic selection is still little used in Software Effort Estimation (SEE). This review adds new evidence about the use of EEE techniques in software development which reinforces previous findings and also shows research opportunities in constructing more effective EEE. Besides, ensemble dynamic selection appears as a promising area of research which still is underexplored.}
}
@article{CHUECA2024107330,
title = {The consolidation of game software engineering: A systematic literature review of software engineering for industry-scale computer games},
journal = {Information and Software Technology},
volume = {165},
pages = {107330},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107330},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923001854},
author = {Jorge Chueca and Javier Verón and Jaime Font and Francisca Pérez and Carlos Cetina},
keywords = {Computer games, Video games, Game software engineering, Systematic literature review, SLR, Industry-scale},
abstract = {Context:
Game Software Engineering (GSE) is a branch of Software Engineering (SE) that focuses on the development of video game applications. In past years, GSE has achieved enough volume, differences from traditional software engineering, and interest by the community to be considered an independent scientific domain, veering out from traditional SE.
Objective:
This study evaluates the current state of the art in software engineering for industry-scale computer games identifying gaps and consolidating the magnitude and growth of this field.
Method:
A Systematic Literature Review is performed following best practices to ensure the relevance of the studies included in the review. We analyzed 98 GSE studies to extract the current intensity, topics, methods, and quality of GSE.
Results:
The GSE research community has been growing over the years, producing over four times more research than before the previous GSE survey. However, this community is still very dispersed, with no main venues holding most of the GSE scientific studies. A broader range of topics is covered in this area, evolving towards those of a mature field such as architecture and design. Also, the reviewed studies employ more elaborated empirical research methods, even though the study reports need to be more rigorous in sections related to the critical examination of the work.
Conclusion:
The results of the SLR lead to the identification of 13 potential future research directions for this domain. GSE is an independent, mature, and growing field that presents new ways of software creation where the gap between industry and academia is narrowing. Video games present themselves as powerful tools to push the boundaries of software knowledge.}
}
@article{GASPARIC2016101,
title = {What recommendation systems for software engineering recommend: A systematic literature review},
journal = {Journal of Systems and Software},
volume = {113},
pages = {101-113},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2015.11.036},
url = {https://www.sciencedirect.com/science/article/pii/S0164121215002605},
author = {Marko Gasparic and Andrea Janes},
keywords = {Recommendation system for software engineering, Systematic literature review},
abstract = {A recommendation system for software engineering (RSSE) is a software application that provides information items estimated to be valuable for a software engineering task in a given context. Present the results of a systematic literature review to reveal the typical functionality offered by existing RSSEs, research gaps, and possible research directions. We evaluated 46 papers studying the benefits, the data requirements, the information and recommendation types, and the effort requirements of RSSE systems. We include papers describing tools that support source code related development published between 2003 and 2013. The results show that RSSEs typically visualize source code artifacts. They aim to improve system quality, make the development process more efficient and less expensive, lower developer’s cognitive load, and help developers to make better decisions. They mainly support reuse actions and debugging, implementation, and maintenance phases. The majority of the systems are reactive. Unexploited opportunities lie in the development of recommender systems outside the source code domain. Furthermore, current RSSE systems use very limited context information and rely on simple models. Context-adapted and proactive behavior could improve the acceptance of RSSE systems in practice.}
}
@article{GARCIAPENALVO20231023,
title = {Explainable Rules and Heuristics in AI Algorithm Recommendation Approaches—A Systematic Literature Review and Mapping Study},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {136},
number = {2},
pages = {1023-1051},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.023897},
url = {https://www.sciencedirect.com/science/article/pii/S152614922300276X},
author = {Francisco José García-Peñalvo and Andrea Vázquez-Ingelmo and Alicia García-Holgado},
keywords = {SLR, systematic literature review, artificial intelligence, machine learning, algorithm recommendation, heuristics, explainability},
abstract = {The exponential use of artificial intelligence (AI) to solve and automated complex tasks has catapulted its popularity generating some challenges that need to be addressed. While AI is a powerful means to discover interesting patterns and obtain predictive models, the use of these algorithms comes with a great responsibility, as an incomplete or unbalanced set of training data or an unproper interpretation of the models’ outcomes could result in misleading conclusions that ultimately could become very dangerous. For these reasons, it is important to rely on expert knowledge when applying these methods. However, not every user can count on this specific expertise; non-AI-expert users could also benefit from applying these powerful algorithms to their domain problems, but they need basic guidelines to obtain the most out of AI models. The goal of this work is to present a systematic review of the literature to analyze studies whose outcomes are explainable rules and heuristics to select suitable AI algorithms given a set of input features. The systematic review follows the methodology proposed by Kitchenham and other authors in the field of software engineering. As a result, 9 papers that tackle AI algorithm recommendation through tangible and traceable rules and heuristics were collected. The reduced number of retrieved papers suggests a lack of reporting explicit rules and heuristics when testing the suitability and performance of AI algorithms.}
}
@article{OLORISADE20171,
title = {Reproducibility of studies on text mining for citation screening in systematic reviews: Evaluation and checklist},
journal = {Journal of Biomedical Informatics},
volume = {73},
pages = {1-13},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417301661},
author = {Babatunde Kazeem Olorisade and Pearl Brereton and Peter Andras},
keywords = {Citation screening, Systematic review, Reproducibility, Text mining, Reproducible research},
abstract = {Context
Independent validation of published scientific results through study replication is a pre-condition for accepting the validity of such results. In computation research, full replication is often unrealistic for independent results validation, therefore, study reproduction has been justified as the minimum acceptable standard to evaluate the validity of scientific claims. The application of text mining techniques to citation screening in the context of systematic literature reviews is a relatively young and growing computational field with high relevance for software engineering, medical research and other fields. However, there is little work so far on reproduction studies in the field.
Objective
In this paper, we investigate the reproducibility of studies in this area based on information contained in published articles and we propose reporting guidelines that could improve reproducibility.
Methods
The study was approached in two ways. Initially we attempted to reproduce results from six studies, which were based on the same raw dataset. Then, based on this experience, we identified steps considered essential to successful reproduction of text mining experiments and characterized them to measure how reproducible is a study given the information provided on these steps. 33 articles were systematically assessed for reproducibility using this approach.
Results
Our work revealed that it is currently difficult if not impossible to independently reproduce the results published in any of the studies investigated. The lack of information about the datasets used limits reproducibility of about 80% of the studies assessed. Also, information about the machine learning algorithms is inadequate in about 27% of the papers. On the plus side, the third party software tools used are mostly free and available.
Conclusions
The reproducibility potential of most of the studies can be significantly improved if more attention is paid to information provided on the datasets used, how they were partitioned and utilized, and how any randomization was controlled. We introduce a checklist of information that needs to be provided in order to ensure that a published study can be reproduced.}
}
@article{CHOMA2025103319,
title = {Investigating quality aspects for UX evaluation of IoT-based applications in smart cities: A literature review},
journal = {Science of Computer Programming},
volume = {245},
pages = {103319},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2025.103319},
url = {https://www.sciencedirect.com/science/article/pii/S0167642325000589},
author = {Joelma Choma and Luciana Zaina},
keywords = {IoT systems, Smart city, User experience, UX evaluation, Quality aspects, Literature review},
abstract = {The Internet of Things (IoT) has increasingly gained prominence in developing smart cities. IoT technologies are essential resources to make smart cities more efficient and sustainable. Recent research in Software Engineering (SE) has investigated the characteristics of IoT systems and the most appropriate approaches to their design and development. The development of systems based on IoT technologies enables a continuous flow of communication in the context of a smart city by allowing different systems to interact and adjust automatically to optimize the city's operation. In an urban environment, IoT connects a vast network of devices such as environmental sensors, public transportation systems, smart traffic lights, security cameras, and more. These characteristics make these applications complex and difficult to evaluate, particularly regarding User Experience (UX) design. Recently, we performed a rapid systematic review to examine the methods and practices commonly employed for evaluating the UX in these scenarios. In our previous work, we analyzed 43 studies covering different types of IoT-based applications and areas of smart cities. In this study, we extend our analysis by exploring which quality aspects have been considered for UX evaluation and categorizing the typical applications evaluated. Our findings revealed the need for more appropriate UX instruments to assess quality aspects that consider specific features of non-traditional interfaces (e.g., haptics, gesture, speech) and smart technologies within specific interaction contexts (e.g., smart environments based on ubiquitous computing). These instruments can be expanded from established guidelines or developed from scratch as long as they are validated in practice.}
}
@article{LWAKATARE2020106368,
title = {Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions},
journal = {Information and Software Technology},
volume = {127},
pages = {106368},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106368},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301373},
author = {Lucy Ellen Lwakatare and Aiswarya Raj and Ivica Crnkovic and Jan Bosch and Helena Holmström Olsson},
keywords = {Machine learning systems, Software engineering, Industrial settings, Challenges, Solutions, SLR},
abstract = {Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems. Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges. Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment. Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions. Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research.}
}
@article{MOLLERI2023111771,
title = {Backsourcing of IT with focus on software development—A systematic literature review},
journal = {Journal of Systems and Software},
volume = {204},
pages = {111771},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111771},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001668},
author = {Jefferson Seide Molléri and Casper Lassenius and Magne Jørgensen},
keywords = {Backsourcing, Software development, Software engineering management, Information technology, Systematic literature review},
abstract = {Context:
Backsourcing is the process of insourcing previously outsourced activities. Backsourcing can be a viable alternative when companies experience environmental or strategic changes, or challenges with outsourcing. While outsourcing and related processes have been extensively studied, few studies report experiences with backsourcing.
Objectives:
We summarize the results of the research literature on backsourcing of IT, with a focus on software development. By identifying practically relevant experience, we present findings that may help companies considering backsourcing. In addition, we identify gaps in the current research literature and point out areas for future work.
Method:
Our systematic literature review (SLR) started with a search for empirical studies on the backsourcing of IT. From each study, we identified the context in which backsourcing occurred, the factors leading to the decision, the backsourcing process, and the outcomes of backsourcing. We employed inductive coding to extract textual data from the papers and qualitative cross-case analysis to synthesize the evidence.
Results:
We identified 17 papers that reported 26 cases of backsourcing, six of which were related to software development. The cases came from a variety of contexts. The most common reasons for backsourcing were improving quality, reducing costs, and regaining control of outsourced activities. We model the backsourcing process as containing five sub-processes: change management, vendor relationship management, competence building, organizational build-up, and transfer of ownership. We identified 14 positive outcomes and nine negative outcomes of backsourcing. We also aggregated the evidence and detailed three relationships of potential use to companies considering backsourcing. Finally, we have highlighted the knowledge areas of software engineering associated with the backsourcing of software development.
Conclusion:
The backsourcing of IT is a complex process; its implementation depends on the prior outsourcing relationship and other contextual factors. Our systematic literature review contributes to a better understanding of this process by identifying its components and their relationships based on the peer-reviewed literature. Our results can serve as a motivation and baseline for further research on backsourcing and provide guidelines and process fragments from which practitioners can benefit when they engage in backsourcing.}
}
@article{GIORDANO2022111475,
title = {On the use of artificial intelligence to deal with privacy in IoT systems: A systematic literature review},
journal = {Journal of Systems and Software},
volume = {193},
pages = {111475},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111475},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001613},
author = {Giammaria Giordano and Fabio Palomba and Filomena Ferrucci},
keywords = {Data privacy, Artificial intelligence, Internet-of-Things, Software engineering for IoT},
abstract = {The Internet of Things (IoT) refers to a network of Internet-enabled devices that can make different operations, like sensing, communicating, and reacting to changes arising in the surrounding environment. Nowadays, the number of IoT devices is already higher than the world population. These devices operate by exchanging data between them, sometimes through an intermediate cloud infrastructure, and may be used to enable a wide variety of novel services that can potentially improve the quality of life of billions of people. Nonetheless, all that glitters is not gold: the increasing adoption of IoT comes with several privacy concerns due to the lack or loss of control over the sensitive data exchanged by these devices. This represents a key challenge for software engineering researchers attempting to address those privacy concerns by proposing (semi-)automated solutions to identify sources of privacy leaks. In this respect, a notable trend is represented by the adoption of smart solutions, that is, the definition of techniques based on artificial intelligence (AI) algorithms. This paper proposes a systematic literature review of the research in smart detection of privacy concerns in IoT devices. Following well-established guidelines, we identify 152 primary studies that we analyze under three main perspectives: (1) What are the privacy concerns addressed with AI-enabled techniques; (2) What are the algorithms employed and how they have been configured/validated; and (3) Which are the domains targeted by these techniques. The key results of the study identified six main tasks targeted through the use of artificial intelligence, like Malware Detection or Network Analysis. Support Vector Machine is the technique most frequently used in literature, however in many cases researchers do not explicitly indicate the domain where to use artificial intelligence algorithms. We conclude the paper by distilling several lessons learned and implications for software engineering researchers.}
}
@article{ALI201948,
title = {A critical appraisal tool for systematic literature reviews in software engineering},
journal = {Information and Software Technology},
volume = {112},
pages = {48-50},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919300771},
author = {Nauman bin Ali and Muhammad Usman},
keywords = {Systematic literature reviews, Quality assessment, Software engineering, Critical appraisal tools, AMSTAR},
abstract = {Context: Methodological research on systematic literature reviews (SLRs) in Software Engineering (SE) has so far focused on developing and evaluating guidelines for conducting systematic reviews. However, the support for quality assessment of completed SLRs has not received the same level of attention. Objective: To raise awareness of the need for a critical appraisal tool (CAT) for assessing the quality of SLRs in SE. To initiate a community-based effort towards the development of such a tool. Method: We reviewed the literature on the quality assessment of SLRs to identify the frequently used CATs in SE and other fields. Results: We identified that the CATs currently used is SE were borrowed from medicine, but have not kept pace with substantial advancements in the field of medicine. Conclusion: In this paper, we have argued the need for a CAT for quality appraisal of SLRs in SE. We have also identified a tool that has the potential for application in SE. Furthermore, we have presented our approach for adapting this state-of-the-art CAT for assessing SLRs in SE.}
}
@incollection{ROZANC2021115,
title = {Chapter Three - The screening phase in systematic reviews: Can we speed up the process?},
editor = {Ali R. Hurson},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {123},
pages = {115-191},
year = {2021},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2021.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0065245821000310},
author = {Igor Rožanc and Marjan Mernik},
keywords = {Software engineering, Systematic review, Systematic literature review, Systematic mapping study, Article screening, Automatic screening process, Tool},
abstract = {The aim of a systematic reviews (SRs) is to gain a better understanding of a certain aspect of selected research field using the principle of classification of a large number of carefully selected articles. Selection of a proper set of articles is a crucial yet delicate task, which demands a large portion of tedious manual work. This article proposes to automate the screening of a large set of articles while conducting an SR. A rigorous approach is described, which conforms with the SR guidelines, and a tool to efficiently support such an approach is presented as well. The effect of approach is presented by a demonstration experiment which compares its results with the results of a classic manual screening. Finally, the recommendations for the proper use of the approach (i.e., the size of the pilot set and decision rule structure) are presented.}
}
@article{KALIBATIENE2026104073,
title = {From manual to automated systematic review: Key attributes influencing the duration of systematic reviews in software engineering},
journal = {Computer Standards & Interfaces},
volume = {96},
pages = {104073},
year = {2026},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2025.104073},
url = {https://www.sciencedirect.com/science/article/pii/S0920548925001023},
author = {Diana Kalibatiene and Jolanta Miliauskaitė},
keywords = {Systematic literature review, Systematic review, Process modelling, Quality, Performance time, Duration, Software engineering research, Process performance measure, Digitalisation},
abstract = {Context
It is widely accepted that a systematic literature review (SLR) is an effective, comprehensive, repeatable, less biased, and transparent method for gathering and condensing knowledge from existing scientific publications. The SLR method facilitates the identification of gaps for new research opportunities, fostering decision-making based on evidence. At the same time, SLR is a time and effort-consuming task that is threatened by the increasing volume of publications. However, there is a lack of comprehension about which factors directly impact manually conducted SLR performance, limiting researchers to better plan and optimize their processes.
Objective
To enhance the understanding of the attributes that directly influence the SLR process in terms of time consumption.
Methods
We performed a tertiary study that (i) identified 138 secondary studies, (ii) mapped the possible influential attributes for SLR performance, (iii) extracted data from SLR reports and metadata, synthesized and analysed their influence, providing an overview of core trends related to those attributes over time.
Results
Our SLR mapped four main attributes influencing the performance time of the SLR process – number of authors, number of initially retrieved papers from databases, number of included papers for data synthesis, and usage of the snowballing techniques. We noticed a trend for smaller research groups (2–5 persons) using 4–6 different databases and processing to process a large number of studies, and an increasing adoption of the snowballing technique.
Conclusion
This paper reveals a bottleneck in manually conducted SLR, reinforcing the need for evolving automation. Mapping the attributes is only the first step to making the SLR process more measurable regarding its resource consumption. We contribute by providing recommendations to assist scientists and practitioners in planning their future SLRs and IT projects, including SLRs, particularly in the evolving landscape of digital transformation and innovations.}
}
@article{DEMAGALHAES201576,
title = {Investigations about replication of empirical studies in software engineering: A systematic mapping study},
journal = {Information and Software Technology},
volume = {64},
pages = {76-101},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000300},
author = {Cleyton V.C. {de Magalhães} and Fabio Q.B. {da Silva} and Ronnie E.S. Santos and Marcos Suassuna},
keywords = {Replications, Experiments, Empirical studies, Mapping study, Systematic literature review, Software engineering},
abstract = {Context
Two recent mapping studies which were intended to verify the current state of replication of empirical studies in Software Engineering (SE) identified two sets of studies: empirical studies actually reporting replications (published between 1994 and 2012) and a second group of studies that are concerned with definitions, classifications, processes, guidelines, and other research topics or themes about replication work in empirical software engineering research (published between 1996 and 2012).
Objective
In this current article, our goal is to analyze and discuss the contents of the second set of studies about replications to increase our understanding of the current state of the work on replication in empirical software engineering research.
Method
We applied the systematic literature review method to build a systematic mapping study, in which the primary studies were collected by two previous mapping studies covering the period 1996–2012 complemented by manual and automatic search procedures that collected articles published in 2013.
Results
We analyzed 37 papers reporting studies about replication published in the last 17years. These papers explore different topics related to concepts and classifications, presented guidelines, and discuss theoretical issues that are relevant for our understanding of replication in our field. We also investigated how these 37 papers have been cited in the 135 replication papers published between 1994 and 2012.
Conclusions
Replication in SE still lacks a set of standardized concepts and terminology, which has a negative impact on the replication work in our field. To improve this situation, it is important that the SE research community engage on an effort to create and evaluate taxonomy, frameworks, guidelines, and methodologies to fully support the development of replications.}
}
@article{SANCHEZGUINEA2016251,
title = {A systematic review on the engineering of software for ubiquitous systems},
journal = {Journal of Systems and Software},
volume = {118},
pages = {251-276},
year = {2016},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2016.05.024},
url = {https://www.sciencedirect.com/science/article/pii/S0164121216300553},
author = {Alejandro {Sánchez Guinea} and Grégory Nain and Yves {Le Traon}},
keywords = {Empirical software engineering, Evidence-based software engineering, Systematic review, Research synthesis, Software development cycle, Ubiquitous systems, Development methods, Pervasive systems},
abstract = {Context: Software engineering for ubiquitous systems has experienced an important and rapid growth, however the vast research corpus makes it difficult to obtain valuable information from it. Objective: To identify, evaluate, and synthesize research about the most relevant approaches addressing the different phases of the software development life cycle for ubiquitous systems. Method: We conducted a systematic literature review of papers presenting and evaluating approaches for the different phases of the software development life cycle for ubiquitous systems. Approaches were classified according to the phase of the development cycle they addressed, identifying their main concerns and limitations. Results: We identified 128 papers reporting 132 approaches addressing issues related to different phases of the software development cycle for ubiquitous systems. Most approaches have been aimed at addressing the implementation, evolution/maintenance, and feedback phases, while others phases such as testing need more attention from researchers. Conclusion: We recommend to follow existing guidelines when conducting case studies to make the studies more reproducible and closer to real life cases. While some phases of the development cycle have been extensively explored, there is still room for research in other phases, toward a more agile and integrated cycle, from requirements to testing and feedback.}
}
@article{VIDONI2022106791,
title = {A systematic process for Mining Software Repositories: Results from a systematic literature review},
journal = {Information and Software Technology},
volume = {144},
pages = {106791},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106791},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921002317},
author = {M. Vidoni},
keywords = {Mining Software Repositories, Systematic literature review, Evidence-based software engineering, Guidelines},
abstract = {Context:
Mining Software Repositories (MSR) is a growing area of Software Engineering (SE) research. Since their emergence in 2004, many investigations have analysed different aspects of these studies. However, there are no guidelines on how to conduct systematic MSR studies. There is a need to evaluate how MSR research is approached to provide a framework to do so systematically.
Objective:
To identify how MSR studies are conducted in terms of repository selection and data extraction. To uncover potential for improvement in directing systematic research and providing guidelines to do so.
Method:
A systematic literature review of MSR studies was conducted following the guidelines and template proposed by Mian et al. (which refines those provided by Kitchenham and Charters). These guidelines were extended and revised to provide a framework for systematic MSR studies.
Results:
MSR studies typically do not follow a systematic approach for repository selection, and many do not report selection or data extraction protocols. Furthermore, few manuscripts discuss threats to the study’s validity due to the selection or data extraction steps followed.
Conclusions:
Although MSR studies are evidence-based research, they seldom follow a systematic process. Hence, there is a need for guidelines on how to conduct systematic MSR studies. New guidelines and a template have been proposed, consolidating related studies in the MSR field and strategies for systematic literature reviews.}
}
@article{NIRMANI2025107753,
title = {A systematic literature review on task recommendation systems for crowdsourced software engineering},
journal = {Information and Software Technology},
volume = {184},
pages = {107753},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107753},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000928},
author = {Shashiwadana Nirmani and Mojtaba Shahin and Hourieh Khalajzadeh and Xiao Liu},
keywords = {Crowdsourced software engineering, Task recommendation, Systematic literature review, GitHub, TopCoder},
abstract = {Context:
Crowdsourced Software Engineering (CSE) offers outsourcing work to software practitioners by leveraging a global online workforce. However, these software practitioners struggle to identify suitable tasks due to the variety of options available. Hence, there have been a growing number of studies on introducing recommendation systems to recommend CSE tasks to software practitioners.
Objective:
The goal of this study is to analyze the existing CSE task recommendation systems, investigating their extracted data, recommendation methods, key advantages and limitations, recommended task types, the use of human factors in recommendations, popular platforms, and features used to make recommendations.
Methods:
This SLR was conducted according to the Kitchenham and Charters’ guidelines. We used manual and automatic search strategies without putting any time limitation for searching the relevant papers.
Results:
We selected 65 primary studies for data extraction, analysis, and synthesis based on our predefined inclusion and exclusion criteria. Based on our data analysis results, we classified the extracted information into four categories according to the data acquisition sources: Software Practitioner’s Profile, Task or Project, Previous Contributions, and Direct Data Collection. We also organized the proposed recommendation systems into a taxonomy and identified key advantages, such as increased performance, accuracy, and optimized solutions. In addition, we identified the limitations of these systems, such as inadequate or biased recommendations and lack of generalizability. Our results revealed that human factors play a major role in CSE task recommendation. Further, we identified five popular task types recommended, popular platforms, and their features used in task recommendation. We also provided recommendations for future research directions.
Conclusion:
This SLR provides insights into current trends, gaps, and future research directions in CSE task recommendation systems such as the need for comprehensive evaluation, standardized evaluation metrics, and benchmarking in future studies, transferring knowledge from other platforms to address cold start problem.}
}
@article{WOHLIN2020106366,
title = {Guidelines for the search strategy to update systematic literature reviews in software engineering},
journal = {Information and Software Technology},
volume = {127},
pages = {106366},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106366},
url = {https://www.sciencedirect.com/science/article/pii/S095058491930223X},
author = {Claes Wohlin and Emilia Mendes and Katia Romero Felizardo and Marcos Kalinowski},
keywords = {Systematic literature review update, Systematic literature reviews, Software engineering, Snowballing, Searching for evidence},
abstract = {Context
Systematic Literature Reviews (SLRs) have been adopted within Software Engineering (SE) for more than a decade to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially not fully up-to-date, and there are no standard proposals on how to update SLRs in SE.
Objective
The objective of this paper is to propose guidelines on how to best search for evidence when updating SLRs in SE, and to evaluate these guidelines using an SLR that was not employed during the formulation of the guidelines.
Method
To propose our guidelines, we compare and discuss outcomes from applying different search strategies to identify primary studies in a published SLR, an SLR update, and two replications in the area of effort estimation. These guidelines are then evaluated using an SLR in the area of software ecosystems, its update and a replication.
Results
The use of a single iteration forward snowballing with Google Scholar, and employing as a seed set the original SLR and its primary studies is the most cost-effective way to search for new evidence when updating SLRs. Furthermore, the importance of having more than one researcher involved in the selection of papers when applying the inclusion and exclusion criteria is highlighted through the results.
Conclusions
Our proposed guidelines formulated based upon an effort estimation SLR, its update and two replications, were supported when using an SLR in the area of software ecosystems, its update and a replication. Therefore, we put forward that our guidelines ought to be adopted for updating SLRs in SE.}
}
@article{GOYAL2025106578,
title = {A systematic review on AI based class imbalance handling in software defect prediction},
journal = {Results in Engineering},
volume = {27},
pages = {106578},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.106578},
url = {https://www.sciencedirect.com/science/article/pii/S2590123025026477},
author = {Somya R. Goyal},
keywords = {Software defect prediction, Systematic literature review, Class imbalance, Sampling, Ensemble learning, Evidence based software engineering},
abstract = {Software defect prediction is to predict the fault-prone modules assisting the development team to focus the testing efforts on the faulty modules, preserving the testing resources and leading to reduced costs of the product. A huge range of diversified AI based mechanisms are available to handle the class imbalance issue underlying the defect datasets that would hinder the performance of prediction models. This study assesses the timeline of evolution of AI applications in the domain of software defect prediction and culminates a Systematic Review over the period of 25 years from 2000 to 2025 dedicatedly focusing on class imbalance issue. To conduct this review, the PRISMA guidelines and statistical hypothesis testing approach are followed. The aim of this study is to assess the AI practices being applied to strengthen prediction models while alleviating the class imbalance. The study about the publication trend in the past 25 years reveals that majority of defect datasets (∼77%) suffer from class imbalances and data sampling is the most popular class imbalance handling technique. Machine learning techniques have been popular and now the paradigm is shifting towards deep models and transfer learning. Explainable AI and LLMs are now the names for defect prediction models over industry datasets. This review provides a holistic view about the defect datasets, learning models, evaluation criteria, class imbalance handling techniques that will pave guidelines and references to the future researchers and software engineers to achieve more reliable and effective software defect prediction models with appropriately adapting the AI techniques to handle the class imbalance conditions.}
}
@article{GUNATILAKE2024107489,
title = {The impact of human aspects on the interactions between software developers and end-users in software engineering: A systematic literature review},
journal = {Information and Software Technology},
volume = {173},
pages = {107489},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107489},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924000946},
author = {Hashini Gunatilake and John Grundy and Rashina Hoda and Ingo Mueller},
keywords = {Systematic literature review, Human aspects, Software developers, Software users, Software engineering},
abstract = {Context:
Research on human aspects within the field of software engineering (SE) has been steadily gaining prominence in recent years. These human aspects have a significant impact on SE due to the inherently interactive and collaborative nature of the discipline.
Objective:
In this paper, we present a systematic literature review (SLR) on human aspects affecting developer-user interactions. The objective of this SLR is to plot the current landscape of primary studies by examining the human aspects that influence developer-user interactions, their implications, interrelationships, and how existing studies address these implications.
Method:
We conducted this SLR following the guidelines proposed by Kitchenham et al. We performed a comprehensive search in six digital databases, and an exhaustive backward and forward snowballing process. We selected 46 primary studies for data extraction.
Results:
We identified various human aspects affecting developer-user interactions in SE, assessed their interrelationships, identified their positive impacts and mitigation strategies for negative effects. We present specific recommendations derived from the identified research gaps.
Conclusion:
Our findings suggest the importance of leveraging positive effects and addressing negative effects in developer-user interactions through the implementation of effective mitigation strategies. These insights may benefit software practitioners for effective user interactions, and the recommendations proposed by this SLR may aid the research community in further human aspects related studies.}
}
@article{KHAN2019396,
title = {Landscaping systematic mapping studies in software engineering: A tertiary study},
journal = {Journal of Systems and Software},
volume = {149},
pages = {396-436},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302784},
author = {Muhammad Uzair Khan and Salman Sherin and Muhammad Zohaib Iqbal and Rubab Zahid},
keywords = {Tertiary study, Systematic mapping study, Secondary study, Survey, Software engineering},
abstract = {Context
A number of Systematic Mapping Studies (SMSs) that cover Software Engineering (SE) are reported in literature. Tertiary studies synthesize the secondary studies to provide a holistic view of an area.
Objectives
We synthesize SMSs in SE to provide insights into existing SE areas and to investigate the trends and quality of SMSs.
Methodology
We use Systematic Literature Review protocol to analyze and map the SMSs in SE, till August 2017, to SE Body of Knowledge (SWEBOK).
Results
We analyze 210 SMSs and results show that: (1) Software design and construction are most active areas in SE; (2) Some areas lack SMSs, including mathematical foundations, software configuration management, and SE tools; (3) The quality of SMSs is improving with time; (4) SMSs in journals have higher quality than SMSs in conferences and are cited more often; (5) Low quality in SMSs can be attributed to a lack of quality assessment in SMSs and not reporting information about the primary studies.
Conclusion
There is a potential for more SMSs in some SE areas. A number of SMSs do not provide the required information for an SMS, which leads to a low quality score.}
}
@article{CRUZBENITO2019118,
title = {Analyzing the software architectures supporting HCI/HMI processes through a systematic review of the literature},
journal = {Telematics and Informatics},
volume = {38},
pages = {118-132},
year = {2019},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2018.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0736585318305392},
author = {Juan Cruz-Benito and Francisco J. García-Peñalvo and Roberto Therón},
keywords = {Human-Computer Interaction, Human-Machine Interaction, Software Architectures, Systematic Literature Review},
abstract = {Many researchers have dealt with Human-Computer Interaction or Human-Machine Interaction by building or designing software architectures that facilitate the users’ interaction or recognize users’ inputs to the generate proper responses. Many studies include these approaches in different research areas: from research in healthcare to mobile environments, robotics, etc. Interaction is seen as a critical concept, and the work for its improvement is a crucial factor for many platforms, systems, and business domains. The goal of this manuscript is to present a systematic review of the literature to identify, analyze and classify the published approaches to support or enhance Human-Computer Interaction or Human-Machine Interaction from the perspective of software architectures. The method followed is the systematic review following the guidelines related to Systematic Literature Reviews methods such as the one proposed by Kitchenham and other authors in the field of software engineering. As results, this study identified 39 papers that included software architectures to improve or analyze Human-Computer Interaction or Human-Machine Interaction. Three main approaches were found on software architectures: layered architectures, modular architectures, and architectures based on software agents, but they lacked standardization and were mainly ad-hoc solutions. The primary interfaces covered were those related to Graphical User Interfaces (GUIs) and multimodal/natural ones. The primary application domain detected were in multimodal systems. The main purpose of most of the papers was to support multimodal interaction. Some conclusions achieved are that the generic solutions to support or analyze HCI/HMI processes are still rare in the literature. Despite many works dealing with this topic and its issues and challenges, it is necessary to keep on improving the research in this area through the application of standard techniques and solutions, exploring new ways of analyzing and interpreting interaction, escaping from ad-hoc solutions or evaluating the solutions proposed.}
}
@article{ZHANG2021106607,
title = {Processes, challenges and recommendations of Gray Literature Review: An experience report},
journal = {Information and Software Technology},
volume = {137},
pages = {106607},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106607},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000847},
author = {He Zhang and Runfeng Mao and Huang Huang and Qiming Dai and Xin Zhou and Haifeng Shen and Guoping Rong},
keywords = {Gray literature review, Evidence-based software engineering, DevSecOps},
abstract = {Context:
Systematic Literature Review (SLR), as a tool of Evidence-Based Software Engineering (EBSE), has been widely used in Software Engineering (SE). However, for certain topics in SE, especially those that are trendy or industry driven, academic literature is generally scarce and consequently Gray Literature (GL) becomes a major source of evidence. In recent years, the adoption of Gray Literature Review (GLR) or Multivocal Literature Review (MLR) is rising steadily to provide the state-of-the-practice of a specific topic where SLR is not a viable option.
Objective:
Although some SLR guidelines recommend the use of GL and several MLR guidelines have already been proposed in SE, researchers still have conflicting views on the value of GL and commonly accepted GLR or MLR studies are generally lacking in terms of publication. This experience report aims to shed some light on GLR through a case study that uses SLR and MLR guidelines to conduct a GLR on an emerging topic in SE to specifically answer the questions related to the reasons of using GL, the processes of conducting GL, and the impacts of GL on review results.
Method:
We retrospect the review process of conducting a GLR on the topic of DevSecOps with reference to Kitchenham’s SLR and Garousi’s MLR guidelines. We specifically reflect on the processes we had to adapt in order to tackle the challenges we faced. We also compare and contrast our GLR with existing MLRs or GLRs in SE to contextualize our reflections.
Results:
We distill ten challenges in nine activities of a GLR process. We provide reasons for these challenges and further suggest ways to tackle them during a GLR process. We also discuss the decision process of selecting a suitable review methodology among SLR, MLR and GLR and elaborate the impacts of GL on our review results.
Conclusion:
Although our experience on GLR is mainly derived from a specific case study on DevSecOps, we conjecture that it is relevant and would be beneficial to other GLR or MLR studies. We also expect our experience would contribute to future GLR or MLR guidelines, in a way similar to how SLR guidelines learned from the SLR experience report a dozen years ago. In addition, other researchers may find our decision making process useful before they conduct their own reviews.}
}
@article{DEMARTINO2025107678,
title = {Classification and challenges of non-functional requirements in ML-enabled systems: A systematic literature review},
journal = {Information and Software Technology},
volume = {181},
pages = {107678},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107678},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000175},
author = {Vincenzo {De Martino} and Fabio Palomba},
keywords = {Software engineering for artificial intelligence, Non-functional requirements, Systematic literature reviews},
abstract = {Context:
Machine learning (ML) is nowadays so pervasive and diffused that virtually no application can avoid its use. Nonetheless, its enormous potential is often tempered by the need to manage non-functional requirements (NFRs) and navigate pressing, contrasting trade-offs.
Objective:
In this respect, we notice a lack of systematic synthesis of challenges explicitly tied to achieving and managing NFRs in ML-enabled systems. Such a synthesis may not only provide a comprehensive summary of the state of the art but also drive further research on the analysis, management, and optimization of NFRS of ML-enabled systems.
Method:
In this paper, we propose a systematic literature review targeting two key aspects such as (1) the classification of the NFRs investigated so far, and (2) the challenges associated with achieving and managing NFRs in ML-enabled systems during model development Through the combination of well-established guidelines for conducting systematic literature reviews and additional search criteria, we survey a total amount of 130 research articles.
Results:
Our findings report that current research identified 31 different NFRs, which can be grouped into six main classes. We also compiled a catalog of 26 software engineering challenges, emphasizing the need for further research to systematically address, prioritize, and balance NFRs in ML-enabled systems.
Conclusion:
We conclude our work by distilling implications and a future outlook on the topic.}
}
@article{NAVEED2024107423,
title = {Model driven engineering for machine learning components: A systematic literature review},
journal = {Information and Software Technology},
volume = {169},
pages = {107423},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107423},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924000284},
author = {Hira Naveed and Chetan Arora and Hourieh Khalajzadeh and John Grundy and Omar Haggag},
keywords = {Model driven engineering, Software engineering, Artificial intelligence, Machine learning, Systematic literature review},
abstract = {Context:
Machine Learning (ML) has become widely adopted as a component in many modern software applications. Due to the large volumes of data available, organizations want to increasingly leverage their data to extract meaningful insights and enhance business profitability. ML components enable predictive capabilities, anomaly detection, recommendation, accurate image and text processing, and informed decision-making. However, developing systems with ML components is not trivial; it requires time, effort, knowledge, and expertise in ML, data processing, and software engineering. There have been several studies on the use of model-driven engineering (MDE) techniques to address these challenges when developing traditional software and cyber–physical systems. Recently, there has been a growing interest in applying MDE for systems with ML components.
Objective:
The goal of this study is to further explore the promising intersection of MDE with ML (MDE4ML) through a systematic literature review (SLR). Through this SLR, we wanted to analyze existing studies, including their motivations, MDE solutions, evaluation techniques, key benefits and limitations.
Method:
Our SLR is conducted following the well-established guidelines by Kitchenham. We started by devising a protocol and systematically searching seven databases, which resulted in 3934 papers. After iterative filtering, we selected 46 highly relevant primary studies for data extraction, synthesis, and reporting.
Results:
We analyzed selected studies with respect to several areas of interest and identified the following: (1) the key motivations behind using MDE4ML; (2) a variety of MDE solutions applied, such as modeling languages, model transformations, tool support, targeted ML aspects, contributions and more; (3) the evaluation techniques and metrics used; and (4) the limitations and directions for future work. We also discuss the gaps in existing literature and provide recommendations for future research.
Conclusion:
This SLR highlights current trends, gaps and future research directions in the field of MDE4ML, benefiting both researchers and practitioners.}
}
@article{ZAKERINASRABADI2023111796,
title = {A systematic literature review on source code similarity measurement and clone detection: Techniques, applications, and challenges},
journal = {Journal of Systems and Software},
volume = {204},
pages = {111796},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111796},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001917},
author = {Morteza Zakeri-Nasrabadi and Saeed Parsa and Mohammad Ramezani and Chanchal Roy and Masoud Ekhtiarzadeh},
keywords = {Source code similarity, Code clone, Plagiarism detection, Code recommendation, Systematic literature review},
abstract = {Measuring and evaluating source code similarity is a fundamental software engineering activity that embraces a broad range of applications, including but not limited to code recommendation, duplicate code, plagiarism, malware, and smell detection. This paper proposes a systematic literature review and meta-analysis on code similarity measurement and evaluation techniques to shed light on the existing approaches and their characteristics in different applications. We initially found over 10,000 articles by querying four digital libraries and ended up with 136 primary studies in the field. The studies were classified according to their methodology, programming languages, datasets, tools, and applications. A deep investigation reveals 80 software tools, working with eight different techniques on five application domains. Nearly 49% of the tools work on Java programs and 37% support C and C++, while there is no support for many programming languages. A noteworthy point was the existence of 12 datasets related to source code similarity measurement and duplicate codes, of which only eight datasets were publicly accessible. The lack of reliable datasets, empirical evaluations, hybrid methods, and focuses on multi-paradigm languages are the main challenges in the field. Emerging applications of code similarity measurement concentrate on the development phase in addition to the maintenance.}
}
@article{NEIVA2016137,
title = {Towards pragmatic interoperability to support collaboration: A systematic review and mapping of the literature},
journal = {Information and Software Technology},
volume = {72},
pages = {137-150},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916000021},
author = {Frâncila Weidt Neiva and José Maria N. David and Regina Braga and Fernanda Campos},
keywords = {Pragmatic interoperability, Collaboration, Collaborative systems, Groupware, Interoperability},
abstract = {Context: Many researchers have argued that providing interoperability support only considering the format and meaning (i.e. syntax and semantic) of data exchange is not enough to achieve complete, effective and meaningful collaboration. Pragmatic interoperability has been highlighted as a key requirement to enhance collaboration. However, fulfilling this requirement is not a trivial task and there is a lack of works discussing solutions to achieve this level of interoperability. Objectives: The aim of this study is to present a systematic review and mapping of the literature in order to identify, analyse and classify the published solutions to achieve pragmatic interoperability. Method: To conduct a systematic review and mapping in accordance with the guidelines proposed in the evidence-based software engineering literature. Results: Our study identified 13 papers reporting pragmatic interoperability computational solutions. The first paper in our set of selected papers was published in 2004; the main strategies used to address pragmatic interoperability issues were service discovery, composition and/or selection and ontologies. The application domain of the identified solutions was mainly e-business. In addition, most of the identified solutions were software architectures. Conclusion: Mature proposals addressing pragmatic interoperability are still rare in the literature. Although many works have discussed the importance of pragmatic interoperability, it is necessary that researchers report solutions that implement and evaluate pragmatic interoperability in order to make progress in this area.}
}
@article{HODA201760,
title = {Systematic literature reviews in agile software development: A tertiary study},
journal = {Information and Software Technology},
volume = {85},
pages = {60-70},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917300538},
author = {Rashina Hoda and Norsaremah Salleh and John Grundy and Hui Mien Tee},
keywords = {Agile software development, Tertiary study, Systematic literature reviews, Mapping study},
abstract = {Context
A number of systematic literature reviews and mapping studies (SLRs) covering numerous primary research studies on various aspects of agile software development (ASD) exist.
Objective
The aim of this paper is to provide an overview of the SLRs on ASD research topics for software engineering researchers and practitioners.
Method
We followed the tertiary study guidelines by Kitchenham et al. to find SLRs published between late 1990s to December 2015.
Results
We found 28 SLRs focusing on ten different ASD research areas: adoption, methods, practices, human and social aspects, CMMI, usability, global software engineering (GSE), organizational agility, embedded systems, and software product line engineering. The number of SLRs on ASD topics, similar to those on software engineering (SE) topics in general, is on the rise. A majority of the SLRs applied standardized guidelines and the quality of these SLRs on ASD topics was found to be slightly higher for journal publications than for conferences. While some individuals and institutions seem to lead this area, the spread of authors and institutions is wide. With respect to prior review recommendations, significant progress was noticed in the area of connecting agile to established domains such as usability, CMMI, and GSE; and considerable progress was observed in focusing on management-oriented approaches as Scrum and sustaining ASD in different contexts such as embedded systems.
Conclusion
SLRs of ASD studies are on the rise and cover a variety of ASD aspects, ranging from early adoption issues to newer applications of ASD such as in product line engineering. ASD research can benefit from further primary and secondary studies on evaluating benefits and challenges of ASD methods, agile hybrids in large-scale setups, sustainability, motivation, teamwork, and project management; as well as a fresh review of empirical studies in ASD to cover the period post 2008.}
}
@article{PONTILLO2024107394,
title = {Test Code Flakiness in Mobile Apps: The Developer’s Perspective},
journal = {Information and Software Technology},
volume = {168},
pages = {107394},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107394},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923002495},
author = {Valeria Pontillo and Fabio Palomba and Filomena Ferrucci},
keywords = {Test Code Flakiness, Software Testing, Mobile Apps Development, Mixed-Method Research},
abstract = {Context:
Test flakiness arises when test cases have a non-deterministic, intermittent behavior that leads them to either pass or fail when run against the same code. While researchers have been contributing to the detection, classification, and removal of flaky tests with several empirical studies and automated techniques, little is known about how the problem of test flakiness arises in mobile applications.
Objective:
We point out a lack of knowledge on: (1) The prominence and harmfulness of the problem; (2) The most frequent root causes inducing flakiness; and (3) The strategies applied by practitioners to deal with it in practice. An improved understanding of these matters may lead the software engineering research community to assess the need for tailoring existing instruments to the mobile context or for brand-new approaches that focus on the peculiarities identified.
Methods:
We address this gap of knowledge by means of an empirical study into the mobile developer’s perception of test flakiness. We first perform a systematic grey literature review to elicit how developers discuss and deal with the problem of test flakiness in the wild. Then, we complement the systematic review through a survey study that involves 130 mobile developers and that aims at analyzing their experience on the matter.
Results:
The results of the grey literature review indicate that developers are often concerned with flakiness connected to user interface elements. In addition, our survey study reveals that flaky tests are perceived as critical by mobile developers, who pointed out major production code- and source code design-related root causes of flakiness, other than the long-term effects of recurrent flaky tests. Furthermore, our study lets the diagnosing and fixing processes currently adopted by developers and their limitations emerge.
Conclusion:
We conclude by distilling lessons learned, implications, and future research directions.}
}
@article{MENDES2020110607,
title = {When to update systematic literature reviews in software engineering},
journal = {Journal of Systems and Software},
volume = {167},
pages = {110607},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110607},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220300856},
author = {Emilia Mendes and Claes Wohlin and Katia Felizardo and Marcos Kalinowski},
keywords = {Systematic literature review update, Systematic literature reviews, Software engineering},
abstract = {[Context] Systematic Literature Reviews (SLRs) have been adopted by the Software Engineering (SE) community for approximately 15 years to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially outdated, and there are no systematic proposals on when to update SLRs in SE. [Objective] The goal of this paper is to provide recommendations on when to update SLRs in SE. [Method] We evaluated, using a three-step approach, a third-party decision framework (3PDF) employed in other fields, to decide whether SLRs need updating. First, we conducted a literature review of SLR updates in SE and contacted the authors to obtain their feedback relating to the usefulness of the 3PDF within the context of SLR updates in SE. Second, we used these authors’ feedback to see whether the framework needed any adaptation; none was suggested. Third, we applied the 3PDF to the SLR updates identified in our literature review. [Results] The 3PDF showed that 14 of the 20 SLRs did not need updating. This supports the use of a decision support mechanism (such as the 3PDF) to help the SE community decide when to update SLRs. [Conclusions] We put forward that the 3PDF should be adopted by the SE community to keep relevant evidence up to date and to avoid wasting effort with unnecessary updates.}
}
@article{STAPLES20071425,
title = {Experiences using systematic review guidelines},
journal = {Journal of Systems and Software},
volume = {80},
number = {9},
pages = {1425-1437},
year = {2007},
note = {Evaluation and Assessment in Software Engineering},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2006.09.046},
url = {https://www.sciencedirect.com/science/article/pii/S0164121206002962},
author = {Mark Staples and Mahmood Niazi},
keywords = {Systematic review, Empirical software engineering},
abstract = {Systematic review is a method to identify, assess and analyse published primary studies to investigate research questions. We critique recently published guidelines for performing systematic reviews on software engineering, and comment on systematic review generally with respect to our experience conducting one. Overall we recommend the guidelines. We recommend researchers clearly and narrowly define research questions to reduce overall effort, and to improve selection and data extraction. We suggest that “complementary” research questions can help clarify the main questions and define selection criteria. We show our project timeline, and discuss possibilities for automating and increasing the acceptance of systematic review.}
}
@article{KITCHENHAM20132049,
title = {A systematic review of systematic review process research in software engineering},
journal = {Information and Software Technology},
volume = {55},
number = {12},
pages = {2049-2075},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913001560},
author = {Barbara Kitchenham and Pearl Brereton},
keywords = {Systematic review, Systematic literature review, Systematic review methodology, Mapping study},
abstract = {Context
Many researchers adopting systematic reviews (SRs) have also published papers discussing problems with the SR methodology and suggestions for improving it. Since guidelines for SRs in software engineering (SE) were last updated in 2007, we believe it is time to investigate whether the guidelines need to be amended in the light of recent research.
Objective
To identify, evaluate and synthesize research published by software engineering researchers concerning their experiences of performing SRs and their proposals for improving the SR process.
Method
We undertook a systematic review of papers reporting experiences of undertaking SRs and/or discussing techniques that could be used to improve the SR process. Studies were classified with respect to the stage in the SR process they addressed, whether they related to education or problems faced by novices and whether they proposed the use of textual analysis tools.
Results
We identified 68 papers reporting 63 unique studies published in SE conferences and journals between 2005 and mid-2012. The most common criticisms of SRs were that they take a long time, that SE digital libraries are not appropriate for broad literature searches and that assessing the quality of empirical studies of different types is difficult.
Conclusion
We recommend removing advice to use structured questions to construct search strings and including advice to use a quasi-gold standard based on a limited manual search to assist the construction of search stings and evaluation of the search process. Textual analysis tools are likely to be useful for inclusion/exclusion decisions and search string construction but require more stringent evaluation. SE researchers would benefit from tools to manage the SR process but existing tools need independent validation. Quality assessment of studies using a variety of empirical methods remains a major problem.}
}
@article{HOISL201749,
title = {Reusable and generic design decisions for developing UML-based domain-specific languages},
journal = {Information and Software Technology},
volume = {92},
pages = {49-74},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304536},
author = {Bernhard Hoisl and Stefan Sobernig and Mark Strembeck},
keywords = {Model-driven software development, Domain-specific language, Design decision, Design rationale, Unified modeling language, Survey},
abstract = {Context: In recent years, UML-based domain-specific model languages (DSMLs) have become a popular option in model-driven development projects. However, making informed design decisions for such DSMLs involves a large number of non-trivial and inter-related options. These options concern the language-model specification, UML extension techniques, concrete-syntax language design, and modeling-tool support. Objective: In order to make the corresponding knowledge on design decisions reusable, proven design rationale from existing DSML projects must be collected, systematized, and documented using an agreed upon documentation format. Method: We applied a sequential multi-method approach to identify and to document reusable design decisions for UML-based DSMLs. The approach included a Web-based survey with 80 participants. Moreover, 80 DSML projects11Note that it is pure coincidence that there were 80 participants in the survey and that 80 DSML projects were reviewed., which have been identified through a prior systematic literature review, were analyzed in detail in order to identify reusable design decisions for such DSMLs. Results: We present insights on the current state of practice in documenting UML-based DSMLs (e.g., perceived barriers, documentation techniques, reuse potential) and a publicly available collection of reusable design decisions, including 35 decision options on different DSML development concerns (especially concerning the language model, concrete-syntax language design, and modeling tools). The reusable design decisions are documented using a structured documentation format (decision record). Conclusion: Our results are both, scientifically relevant (e.g. for design-space analyses or for creating classification schemas for further research on UML-based DSML development) and important for actual software engineering projects (e.g. by providing best-practice guidelines and pointers to common pitfalls).}
}
@article{DASILVA2011899,
title = {Six years of systematic literature reviews in software engineering: An updated tertiary study},
journal = {Information and Software Technology},
volume = {53},
number = {9},
pages = {899-913},
year = {2011},
note = {Studying work practices in Global Software Engineering},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2011.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584911001017},
author = {Fabio Q.B. {da Silva} and André L.M. Santos and Sérgio Soares and A. César C. França and Cleviton V.F. Monteiro and Felipe Farias Maciel},
keywords = {Systematic reviews, Mapping studies, Software engineering, Tertiary studies},
abstract = {Context
Since the introduction of evidence-based software engineering in 2004, systematic literature review (SLR) has been increasingly used as a method for conducting secondary studies in software engineering. Two tertiary studies, published in 2009 and 2010, identified and analysed 54 SLRs published in journals and conferences in the period between 1st January 2004 and 30th June 2008.
Objective
In this article, our goal was to extend and update the two previous tertiary studies to cover the period between 1st July 2008 and 31st December 2009. We analysed the quality, coverage of software engineering topics, and potential impact of published SLRs for education and practice.
Method
We performed automatic and manual searches for SLRs published in journals and conference proceedings, analysed the relevant studies, and compared and integrated our findings with the two previous tertiary studies.
Results
We found 67 new SLRs addressing 24 software engineering topics. Among these studies, 15 were considered relevant to the undergraduate educational curriculum, and 40 appeared of possible interest to practitioners. We found that the number of SLRs in software engineering is increasing, the overall quality of the studies is improving, and the number of researchers and research organisations worldwide that are conducting SLRs is also increasing and spreading.
Conclusion
Our findings suggest that the software engineering research community is starting to adopt SLRs consistently as a research method. However, the majority of the SLRs did not evaluate the quality of primary studies and fail to provide guidelines for practitioners, thus decreasing their potential impact on software engineering practice.}
}
@article{BUDGEN2022106840,
title = {Short communication: Evolution of secondary studies in software engineering},
journal = {Information and Software Technology},
volume = {145},
pages = {106840},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106840},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000179},
author = {David Budgen and Pearl Brereton},
keywords = {Systematic review, Mapping study, Qualitative study, Experience of authors},
abstract = {Context:
Other disciplines commonly employ secondary studies to address the needs of practitioners and policy-makers. Since being adopted by software engineering in 2004, many have been undertaken by researchers.
Objective:
To assess how the role of secondary studies in software engineering has evolved.
Methods:
We examined a sample of 131 secondary studies published in a set of five major software engineering journals for the years 2010, 2015 and 2020. These were categorised by their type (e.g. mapping study), their research focus (quantitative/qualitative and practice/methodological), as well as the experience of the first authors.
Results:
Secondary studies are now a well-established research tool. They are predominantly qualitative and there is extensive use of mapping studies to profile research in particular areas. A significant number are clearly produced as part of postgraduate study, although experienced researchers also conduct many secondary studies. They are sometimes also used as part of a multi-method study.
Conclusion:
Existing guidelines largely focus upon quantitative systematic reviews. Based on our findings, we suggest that more guidance is needed on how to conduct, analyse, and report qualitative secondary studies.}
}
@article{ALI2018133,
title = {Reliability of search in systematic reviews: Towards a quality assessment framework for the automated-search strategy},
journal = {Information and Software Technology},
volume = {99},
pages = {133-147},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304263},
author = {Nauman Bin Ali and Muhammad Usman},
keywords = {Secondary studies, Systematic literature reviews, Search strategies, Reliability, Credibility, Guidelines},
abstract = {Context
The trust in systematic literature reviews (SLRs) to provide credible recommendations is critical for establishing evidence-based software engineering (EBSE) practice. The reliability of SLR as a method is not a given and largely depends on the rigor of the attempt to identify, appraise and aggregate evidence. Previous research, by comparing SLRs on the same topic, has identified search as one of the reasons for discrepancies in the included primary studies. This affects the reliability of an SLR, as the papers identified and included in it are likely to influence its conclusions.
Objective
We aim to propose a comprehensive evaluation checklist to assess the reliability of an automated-search strategy used in an SLR.
Method
Using a literature review, we identified guidelines for designing and reporting automated-search as a primary search strategy. Using the aggregated design, reporting and evaluation guidelines, we formulated a comprehensive evaluation checklist. The value of this checklist was demonstrated by assessing the reliability of search in 27 recent SLRs.
Results
Using the proposed evaluation checklist, several additional issues (not captured by the current evaluation checklist) related to the reliability of search in recent SLRs were identified. These issues severely limit the coverage of literature by the search and also the possibility to replicate it.
Conclusion
Instead of solely relying on expensive replications to assess the reliability of SLRs, this work provides means to objectively assess the likely reliability of a search-strategy used in an SLR. It highlights the often-assumed aspect of repeatability of search when using automated-search. Furthermore, by explicitly considering repeatability and consistency as sub-characteristics of a reliable search, it provides a more comprehensive evaluation checklist than the ones currently used in EBSE.}
}
@article{BJORNSON20081055,
title = {Knowledge management in software engineering: A systematic review of studied concepts, findings and research methods used},
journal = {Information and Software Technology},
volume = {50},
number = {11},
pages = {1055-1068},
year = {2008},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2008.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584908000487},
author = {Finn Olav Bjørnson and Torgeir Dingsøyr},
keywords = {Software engineering, Knowledge management, Learning software organization, Software process improvement, Systematic review},
abstract = {Software engineering is knowledge-intensive work, and how to manage software engineering knowledge has received much attention. This systematic review identifies empirical studies of knowledge management initiatives in software engineering, and discusses the concepts studied, the major findings, and the research methods used. Seven hundred and sixty-two articles were identified, of which 68 were studies in an industry context. Of these, 29 were empirical studies and 39 reports of lessons learned. More than half of the empirical studies were case studies. The majority of empirical studies relate to technocratic and behavioural aspects of knowledge management, while there are few studies relating to economic, spatial and cartographic approaches. A finding reported across multiple papers was the need to not focus exclusively on explicit knowledge, but also consider tacit knowledge. We also describe implications for research and for practice.}
}
@article{ZHANG2011625,
title = {Identifying relevant studies in software engineering},
journal = {Information and Software Technology},
volume = {53},
number = {6},
pages = {625-637},
year = {2011},
note = {Special Section: Best papers from the APSEC},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002260},
author = {He Zhang and Muhammad Ali Babar and Paolo Tell},
keywords = {Search strategy, Quasi-gold standard, Systematic literature review, Evidence-based software engineering},
abstract = {Context
Systematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries.
Objective
The main objective of the research reported in this paper is to improve the search step of undertaking SLRs in software engineering (SE) by devising and evaluating systematic and practical approaches to identifying relevant studies in SE.
Method
We have systematically selected and analytically studied a large number of papers (SLRs) to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLRs, we have devised a systematic and evidence-based approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of ‘quasi-gold standard’ (QGS), which consists of collection of known studies, and corresponding ‘quasi-sensitivity’ into the search process for evaluating search performance.
Results
We conducted two participant–observer case studies to demonstrate and evaluate the adoption of the proposed QGS-based systematic search approach in support of SLRs in SE research.
Conclusion
We report their findings based on the case studies that the approach is able to improve the rigor of search process in an SLR, as well as it can serve as a supplement to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using a series of case studies on varying research topics in SE.}
}
@article{DOSSANTOS2024107551,
title = {Sustainable systematic literature reviews},
journal = {Information and Software Technology},
volume = {176},
pages = {107551},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107551},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924001563},
author = {Vinicius {dos Santos} and Anderson Y. Iwazaki and Katia R. Felizardo and Érica F. {de Souza} and Elisa Y. Nakagawa},
keywords = {Secondary study, Systematic literature review, SLR, Sustainability},
abstract = {Context:
Systematic Literature Reviews (SLR) have been recognized as an important research method for summarizing evidence in Software Engineering (SE). At the same, SLR still presents several problems, such as the high resource consumption (mainly human resources) and lack of effective impact on SE practitioners, although much research has already been done.
Objective:
The main goal of this paper is to explore the concept of sustainability in the SLR area, intending to contribute to understanding better and solving such problems in an integrated way. More specifically, this paper characterizes what sustainable SLR are, their core characteristics, critical factors (i.e., sensitive points in the SLR process), and guidelines for conducting such SLR.
Methods:
We performed a meta-ethnographic study to find key concepts of sustainable software systems and transpose them to sustainable SLR. For this, we systematically selected 16 studies about sustainable software systems and 14 distinguished studies about SLR. Following, we extracted the main keywords and metaphors, determined how both areas are correlated, and transposed them to obtain a set of core characteristics of sustainable SLR as well as critical factors and guidelines. Additionally, we validated them with specialists using the Delphi method.
Results:
We found 15 core characteristics that offer a broad view of sustainable SLR, 15 critical factors in the SLR process that should be carefully addressed when conducting and updating SLR, and also 16 guidelines to manage SLR from the sustainability perspective.
Conclusion:
The concept of sustainability in SLR can contribute to solving SLR problems in a more integrated way, while this work could change the mindset of the SLR community about the need to conduct sustainable SLR.}
}
@article{GAROUSI2016106,
title = {Challenges and best practices in industry-academia collaborations in software engineering: A systematic literature review},
journal = {Information and Software Technology},
volume = {79},
pages = {106-127},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916301203},
author = {Vahid Garousi and Kai Petersen and Baris Ozkan},
keywords = {Software engineering, Industry-academia collaborations, Industry, Universities, Challenges, Success patterns, Best practices, Systematic literature review},
abstract = {Context: The global software industry and the software engineering (SE) academia are two large communities. However, unfortunately, the level of joint industry-academia collaborations in SE is still relatively very low, compared to the amount of activity in each of the two communities. It seems that the two ’camps’ show only limited interest/motivation to collaborate with one other. Many researchers and practitioners have written about the challenges, success patterns (what to do, i.e., how to collaborate) and anti-patterns (what not do do) for industry-academia collaborations. Objective: To identify (a) the challenges to avoid risks to the collaboration by being aware of the challenges, (b) the best practices to provide an inventory of practices (patterns) allowing for an informed choice of practices to use when planning and conducting collaborative projects. Method: A systematic review has been conducted. Synthesis has been done using grounded-theory based coding procedures. Results: Through thematic analysis we identified 10 challenge themes and 17 best practice themes. A key outcome was the inventory of best practices, the most common ones recommended in different contexts were to hold regular workshops and seminars with industry, assure continuous learning from industry and academic sides, ensure management engagement, the need for a champion, basing research on real-world problems, showing explicit benefits to the industry partner, be agile during the collaboration, and the co-location of the researcher on the industry side. Conclusion: Given the importance of industry-academia collaboration to conduct research of high practical relevance we provide a synthesis of challenges and best practices, which can be used by researchers and practitioners to make informed decisions on how to structure their collaborations.}
}
@article{CLEMMENSEN2025112348,
title = {Cyber-physical systems with Human-in-the-Loop: A systematic review of socio-technical perspectives},
journal = {Journal of Systems and Software},
volume = {226},
pages = {112348},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112348},
url = {https://www.sciencedirect.com/science/article/pii/S0164121225000160},
author = {Torkil Clemmensen and Mahyar Tourchi Moghaddam and Jacob Nørbjerg},
keywords = {Cyber-physical systems, Human-in-the-Loop, Socio-technical},
abstract = {Understanding and designing Cyber-physical systems (CPS) with humans in the loop (HITL) is a basic cross-scientific research problem with large implications for industry. The current software engineering knowledge already explains how to include the humans in the operation of the machines in terms of interfaces, architectures, adaptive systems, and design methodologies for including the Human-in-the-Loop. This paper extends existing knowledge with a systematic review of socio-technical perspectives on CPS with HITL. The review was software engineering focused, as it searched the body of research on CPS with HITL, and only within that body, those papers that included socio-technical perspectives. The results indicated four main areas in the ST literature. Validating these insights by expert interviews with industry CPS experts showed some alignment and also fundamental differences between the socio-technical literature (ST literature) insights and the industry experts’ viewpoints. The discussion identifies useful crossings between the ST literature and research into CPS with HITL adaption, and touch on the issues of non-alignments in industry practice. The conclusion is that the ST perspectives on the body of knowledge on CPS with HITL has much to offer researchers in terms of innovative ways to look at the HITL, but the literature needs further development before industry experts can effectively use it. Future research possibilities are outlined.}
}
@article{BUDGEN201862,
title = {Reporting systematic reviews: Some lessons from a tertiary study},
journal = {Information and Software Technology},
volume = {95},
pages = {62-74},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916303548},
author = {David Budgen and Pearl Brereton and Sarah Drummond and Nikki Williams},
keywords = {Systematic review, Reporting quality, Provenance of findings},
abstract = {Context
Many of the systematic reviews published in software engineering are related to research or methodological issues and hence are unlikely to be of direct benefit to practitioners or teachers. Those that are relevant to practice and teaching need to be presented in a form that makes their findings usable with minimum interpretation.
Objective
We have examined a sample of the many systematic reviews that have been published over a period of six years, in order to assess how well these are reported and identify useful lessons about how this might be done.
Method
We undertook a tertiary study, performing a systematic review of systematic reviews. Our study found 178 systematic reviews published in a set of major software engineering journals over the period 2010–2015. Of these, 37 provided recommendations or conclusions of relevance to education and/or practice and we used the DARE criteria as well as other attributes related to the systematic review process to analyse how well they were reported.
Results
We have derived a set of 12 ‘lessons’ that could help authors with reporting the outcomes of a systematic review in software engineering. We also provide an associated checklist for use by journal and conference referees.
Conclusion
There are several areas where better reporting is needed, including quality assessment, synthesis, and the procedures followed by the reviewers. Researchers, practitioners, teachers and journal referees would all benefit from better reporting of systematic reviews, both for clarity and also for establishing the provenance of any findings.}
}
@article{KITCHENHAM2010792,
title = {Systematic literature reviews in software engineering – A tertiary study},
journal = {Information and Software Technology},
volume = {52},
number = {8},
pages = {792-805},
year = {2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910000467},
author = {Barbara Kitchenham and Rialette Pretorius and David Budgen and O. {Pearl Brereton} and Mark Turner and Mahmood Niazi and Stephen Linkman},
keywords = {Systematic literature review, Mapping study, Software engineering, Tertiary study},
abstract = {Context
In a previous study, we reported on a systematic literature review (SLR), based on a manual search of 13 journals and conferences undertaken in the period 1st January 2004 to 30th June 2007.
Objective
The aim of this on-going research is to provide an annotated catalogue of SLRs available to software engineering researchers and practitioners. This study updates our previous study using a broad automated search.
Method
We performed a broad automated search to find SLRs published in the time period 1st January 2004 to 30th June 2008. We contrast the number, quality and source of these SLRs with SLRs found in the original study.
Results
Our broad search found an additional 35 SLRs corresponding to 33 unique studies. Of these papers, 17 appeared relevant to the undergraduate educational curriculum and 12 appeared of possible interest to practitioners. The number of SLRs being published is increasing. The quality of papers in conferences and workshops has improved as more researchers use SLR guidelines.
Conclusion
SLRs appear to have gone past the stage of being used solely by innovators but cannot yet be considered a main stream software engineering research methodology. They are addressing a wide range of topics but still have limitations, such as often failing to assess primary study quality.}
}
@article{PAIVA2021110819,
title = {Accessibility and Software Engineering Processes: A Systematic Literature Review},
journal = {Journal of Systems and Software},
volume = {171},
pages = {110819},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110819},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220302168},
author = {Débora Maria Barroso Paiva and André Pimenta Freire and Renata Pontin {de Mattos Fortes}},
keywords = {Accessibility, Software Engineering, Systematic Literature Review, Design for disabilities, Methods for accessibility},
abstract = {Guidelines, techniques, and methods have been presented in the literature in recent years to contribute to the development of accessible software and to promote digital inclusion. Considering that software product quality depends on the quality of the development process, researchers have investigated how to include accessibility during the software development process in order to obtain accessible software. Two Systematic Literature Reviews (SLR) have been conducted in the past to identify such research initiatives. This paper presents a new SLR, considering the period from 2011 to 2019. The review of 94 primary studies showed the distribution of publications on different phases of the software life cycle, mainly the design and testing phases. The study also identified, for the first time, papers about accessibility and software process establishment. This result reinforces that, in fact, accessibility is not characterized as a property of the final software only. Instead, it evolves over the software life cycle. Besides, this study aims to provide designers and developers with an updated view of methods, tools, and other assets that contribute to process enrichment, valuing accessibility, as well as shows the gaps and challenges which deserve to be investigated.}
}
@article{PERALGARCIA2024100619,
title = {Systematic literature review: Quantum machine learning and its applications},
journal = {Computer Science Review},
volume = {51},
pages = {100619},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100619},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000030},
author = {David Peral-García and Juan Cruz-Benito and Francisco José García-Peñalvo},
keywords = {Quantum machine learning, Quantum computing, Systematic literature review},
abstract = {Quantum physics has changed the way we understand our environment, and one of its branches, quantum mechanics, has demonstrated accurate and consistent theoretical results. Quantum computing is the process of performing calculations using quantum mechanics. This field studies the quantum behavior of certain subatomic particles (photons, electrons, etc.) for subsequent use in performing calculations, as well as for large-scale information processing. These advantages are achieved through the use of quantum features, such as entanglement or superposition. These capabilities can give quantum computers an advantage in terms of computational time and cost over classical computers. Nowadays, scientific challenges are impossible to perform by classical computation due to computational complexity (more bytes than atoms in the observable universe) or the time it would take (thousands of years), and quantum computation is the only known answer. However, current quantum devices do not have yet the necessary qubits and are not fault-tolerant enough to achieve these goals. Nonetheless, there are other fields like machine learning, finance, or chemistry where quantum computation could be useful with current quantum devices. This manuscript aims to present a review of the literature published between 2017 and 2023 to identify, analyze, and classify the different types of algorithms used in quantum machine learning and their applications. The methodology follows the guidelines related to Systematic Literature Review methods, such as the one proposed by Kitchenham and other authors in the software engineering field. Consequently, this study identified 94 articles that used quantum machine learning techniques and algorithms and shows their implementation using computational quantum circuits or ansatzs. The main types of found algorithms are quantum implementations of classical machine learning algorithms, such as support vector machines or the k-nearest neighbor model, and classical deep learning algorithms, like quantum neural networks. One of the most relevant applications in the machine learning field is image classification. Many articles, especially within the classification, try to solve problems currently answered by classical machine learning but using quantum devices and algorithms. Even though results are promising, quantum machine learning is far from achieving its full potential. An improvement in quantum hardware is required for this potential to be achieved since the existing quantum computers lack enough quality, speed, and scale to allow quantum computing to achieve its full potential.}
}
@article{FEBRERO201618,
title = {Software reliability modeling based on ISO/IEC SQuaRE},
journal = {Information and Software Technology},
volume = {70},
pages = {18-29},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915001652},
author = {Felipe Febrero and Coral Calero and M. {Ángeles Moraga}},
keywords = {Software quality, Software reliability modeling, International standard, SQuaRE},
abstract = {Context
The increasing dependence of our society on software driven systems has led Software Reliability to become a key factor as well as making it a highly active research area with hundreds of works being published every year. It would, however, appear that this activity is much more reduced as regards how to apply representative international standards on Product Quality to industrial environments, with just a few works on Standard Based software reliability modeling (SB-SRM). This is surprising given the relevance of such International Standards in industry.
Objective
To identify and analyze the existing works on the modeling of Software Reliability based on International Standards as the starting point for a reliability assessment proposal based on ISO/IEC-25000 “Software Product Quality Requirements and Evaluation” (SQuaRE) series.
Method
The work methodology is based on the guidelines provided in Evidence Based Software Engineering for Systematic Literature Reviews (SLR).
Results
A total of 1820 works were obtained as a result of the SLR search, more than 800 primary studies were selected after data filtering. After scrutiny, over thirty of those were thoroughly analyze, the results obtained show a very limited application of SB-SRM particularly to industrial environment.
Conclusion
Our analysis point to the complexity of the proposed models together with the difficulties involved in applying them to the management of engineering activities as a root cause to be considered for such limited application. The various stakeholder needs are also a point of paramount importance that should be better covered if the industrial applicability of the proposed models is to be increased.}
}
@article{DEHDARIRAD2020169,
title = {Scholarly publication venue recommender systems},
journal = {Data Technologies and Applications},
volume = {54},
number = {2},
pages = {169-191},
year = {2020},
issn = {2514-9288},
doi = {https://doi.org/10.1108/DTA-08-2019-0135},
url = {https://www.sciencedirect.com/science/article/pii/S2514928820000474},
author = {Hossein Dehdarirad and Javad Ghazimirsaeid and Ammar Jalalimanesh},
keywords = {Recommender systems, Recommendation systems, Venue, Journal, Conference},
abstract = {Purpose
The purpose of this investigation is to identify, evaluate, integrate and summarize relevant and qualified papers through conducting a systematic literature review (SLR) on the application of recommender systems (RSs) to suggest a scholarly publication venue for researcher's paper.
Design/methodology/approach
To identify the relevant papers published up to August 11, 2018, an SLR study on four databases (Scopus, Web of Science, IEEE Xplore and ScienceDirect) was conducted. We pursued the guidelines presented by Kitchenham and Charters (2007) for performing SLRs in software engineering. The papers were analyzed based on data sources, RSs classes, techniques/methods/algorithms, datasets, evaluation methodologies and metrics, as well as future directions.
Findings
A total of 32 papers were identified. The most data sources exploited in these papers were textual (title/abstract/keywords) and co-authorship data. The RS classes in the selected papers were almost equally used. DBLP was the main dataset utilized. Cosine similarity, social network analysis (SNA) and term frequency–inverse document frequency (TF–IDF) algorithm were frequently used. In terms of evaluation methodologies, 24 papers applied only offline evaluations. Furthermore, precision, accuracy and recall metrics were the popular performance metrics. In the reviewed papers, “use more datasets” and “new algorithms” were frequently mentioned in the future work part as well as conclusions.
Originality/value
Given that a review study has not been conducted in this area, this paper can provide an insight into the current status in this area and may also contribute to future research in this field.}
}
@article{SHAMSUJJOHA2021106693,
title = {Developing Mobile Applications Via Model Driven Development: A Systematic Literature Review},
journal = {Information and Software Technology},
volume = {140},
pages = {106693},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106693},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921001488},
author = {Md. Shamsujjoha and John Grundy and Li Li and Hourieh Khalajzadeh and Qinghua Lu},
keywords = {Systematic Literature Review, Model Driven Development, Mobile App, Tools and Techniques},
abstract = {Context:
Mobile applications (known as “apps”) usage continues to rapidly increase, with many new apps being developed and deployed. However, developing a mobile app is challenging due to its dependencies on devices, technologies, platforms, and deadlines to reach the market. One potential approach is to use Model Driven Development (MDD) techniques that simplify the app development process, reduce complexity, increase abstraction level, help achieve scalable solutions and maximize cost-effectiveness and productivity.
Objective:
This paper systematically investigates what MDD techniques and methodologies have been used to date to support mobile app development and how these techniques have been employed, to identify key benefits, limitations, gaps and future research potential.
Method:
A Systematic Literature Review approach was used for this study based on a formal protocol. The rigorous search protocol identified a total of 1,042 peer-reviewed academic research papers from four major software engineering databases. These papers were subsequently filtered, and 55 high quality relevant studies were selected for analysis, synthesis, and reporting.
Results:
We identified the popularity of different applied MDD approaches, supporting tools, artifacts, and evaluation techniques. Our analysis found that architecture, domain model, and code generation are the most crucial purposes in MDD-based app development. Three qualities – productivity, scalability and reliability – can benefit from these modeling strategies. We then summarize the key collective strengths, limitations, gaps from the studies and made several future recommendations.
Conclusion:
There has been a steady interest in MDD approaches applied to mobile app development over the years. This paper guides future researchers, developers, and stakeholders to improve app development techniques, ultimately that will help end-users in having more effective apps, especially when some recommendations are addressed, e.g., taking into account more human-centric aspects in app development.}
}
@article{PETERSEN20151,
title = {Guidelines for conducting systematic mapping studies in software engineering: An update},
journal = {Information and Software Technology},
volume = {64},
pages = {1-18},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000646},
author = {Kai Petersen and Sairam Vakkalanka and Ludwik Kuzniarz},
keywords = {Systematic mapping studies, Software engineering, Guidelines},
abstract = {Context
Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines.
Objective
To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly.
Method
We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment).
Results
In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given.
Conclusion
The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings.}
}
@article{STRADOWSKI2023107192,
title = {Industrial applications of software defect prediction using machine learning: A business-driven systematic literature review},
journal = {Information and Software Technology},
volume = {159},
pages = {107192},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107192},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923000460},
author = {Szymon Stradowski and Lech Madeyski},
keywords = {Software defect prediction, Machine learning, Systematic literature review, Effort and cost minimisation, Real-world, Industry},
abstract = {Context:
Machine learning software defect prediction is a promising field of software engineering, attracting a great deal of attention from the research community; however, its industry application tents to lag behind academic achievements.
Objective:
This study is part of a larger project focused on improving the quality and minimising the cost of software testing of the 5G system at Nokia, and aims to evaluate the business applicability of machine learning software defect prediction and gather lessons learnt.
Methods:
The systematic literature review was conducted on journal and conference papers published between 2015 and 2022 in popular online databases (ACM, IEEE, Springer, Scopus, Science Direct, and Google Scholar). A quasi-gold standard procedure was used to validate the search, and SEGRESS guidelines were used for transparency, reporting, and replicability.
Results:
We have selected and analysed 32 publications out of 397 found by our automatic search (and seven by snowballing). We have identified highly relevant evidence of methods, features, frameworks, and datasets used. However, we found a minimal emphasis on practical lessons learnt and cost consciousness — both vital from a business perspective.
Conclusion:
Even though the number of machine learning software defect prediction studies validated in the industry is increasing (and we were able to identify several excellent papers on studies performed in vivo), there is still not enough practical focus on the business aspects of the effort that would help bridge the gap between the needs of the industry and academic research.}
}
@article{KINAST2023,
title = {Functional Requirements for Medical Data Integration into Knowledge Management Environments: Requirements Elicitation Approach Based on Systematic Literature Analysis},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/41344},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123000948},
author = {Benjamin Kinast and Hannes Ulrich and Björn Bergh and Björn Schreiweis},
keywords = {data integration, requirements engineering, requirements, knowledge management, software engineering},
abstract = {Background
In patient care, data are historically generated and stored in heterogeneous databases that are domain specific and often noninteroperable or isolated. As the amount of health data increases, the number of isolated data silos is also expected to grow, limiting the accessibility of the collected data. Medical informatics is developing ways to move from siloed data to a more harmonized arrangement in information architectures. This paradigm shift will allow future research to integrate medical data at various levels and from various sources. Currently, comprehensive requirements engineering is working on data integration projects in both patient care– and research-oriented contexts, and it is significantly contributing to the success of such projects. In addition to various stakeholder-based methods, document-based requirement elicitation is a valid method for improving the scope and quality of requirements.
Objective
Our main objective was to provide a general catalog of functional requirements for integrating medical data into knowledge management environments. We aimed to identify where integration projects intersect to derive consistent and representative functional requirements from the literature. On the basis of these findings, we identified which functional requirements for data integration exist in the literature and thus provide a general catalog of requirements.
Methods
This work began by conducting a literature-based requirement elicitation based on a broad requirement engineering approach. Thus, in the first step, we performed a web-based systematic literature review to identify published articles that dealt with the requirements for medical data integration. We identified and analyzed the available literature by applying the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. In the second step, we screened the results for functional requirements using the requirements engineering method of document analysis and derived the requirements into a uniform requirement syntax. Finally, we classified the elicited requirements into a category scheme that represents the data life cycle.
Results
Our 2-step requirements elicitation approach yielded 821 articles, of which 61 (7.4%) were included in the requirement elicitation process. There, we identified 220 requirements, which were covered by 314 references. We assigned the requirements to different data life cycle categories as follows: 25% (55/220) to data acquisition, 35.9% (79/220) to data processing, 12.7% (28/220) to data storage, 9.1% (20/220) to data analysis, 6.4% (14/220) to metadata management, 2.3% (5/220) to data lineage, 3.2% (7/220) to data traceability, and 5.5% (12/220) to data security.
Conclusions
The aim of this study was to present a cross-section of functional data integration–related requirements defined in the literature by other researchers. The aim was achieved with 220 distinct requirements from 61 publications. We concluded that scientific publications are, in principle, a reliable source of information for functional requirements with respect to medical data integration. Finally, we provide a broad catalog to support other scientists in the requirement elicitation phase.}
}
@article{DIXIT2022100314,
title = {Towards user-centered and legally relevant smart-contract development: A systematic literature review},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100314},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100314},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21001072},
author = {Abhishek Dixit and Vipin Deval and Vimal Dwivedi and Alex Norta and Dirk Draheim},
keywords = {Blockchain, Smart contract, Ricardian contract, Business collaboration, Legal relevance},
abstract = {Smart contracts (SC) run on blockchain technology (BCT) to implement agreements between several parties. As BCT grows, organizations aim to automate their processes and engage in business collaborations using SCs. The translation of contract semantics into SC language semantics is difficult due to ambiguous contractual interpretation by the several parties and the developers. Also, an SC language itself misses the language constructs needed for semantically expressing collaboration terms. This leads to SC coding errors that result in contractual conflicts over transactions during the performance of SCs and thus, novel SC solutions incur high development and maintenance costs. Various model-based and no/low code development approaches address this issue by enabling higher abstractions in SC development. Still, the question remains unanswered how contractual parties, i.e., end-users with non-IT skills, manage to develop legally relevant SCs with ease. This study aims to (1) identify and categorize the state of the art of SC automation models, in terms of their technical features, and their legal significance, and to (2) identify new research opportunities. The review has been conducted as a systematic literature review (SLR) that follows the guidelines proposed by Kitchenham for performing SLRs in software-engineering. As a result of the implementation of the review protocol, 1367 papers are collected, and 33 of them are selected for extraction and analysis. The contributions of this article are threefold: (1) 10 different SC automation models/frameworks are identified and classified according to their technical and implementation features; (2) 11 different legal contract parameters are identified and categorized into 4 legal criteria classes; (3) a comparative analysis of SC-automation models in the context of their legal significance is conducted that identifies the degrees to which the SC-automation models are considered legally relevant. As a conclusion, we produce a comprehensive and replicable overview of the state of the art of SC automation models and a systematic measure of their legal significance to benefit practitioners in the field.}
}
@article{CABALLEROESPINOSA2023107078,
title = {Community smells—The sources of social debt: A systematic literature review},
journal = {Information and Software Technology},
volume = {153},
pages = {107078},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107078},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001872},
author = {Eduardo Caballero-Espinosa and Jeffrey C. Carver and Kimberly Stowers},
keywords = {Community smells, Social debt, Software development teams, Systematic literature review, Teamwork, Team performance},
abstract = {Context:
Social debt describes the accumulation of unforeseen project costs (or potential costs) from sub-optimal software development processes. Community smells are sociotechnical anti-patterns and one source of social debt. Because community smells impact software teams, development processes, outcomes, and organizations, we to understand their impact on software engineering.
Objective:
To provide an overview of community smells in social debt, based on published literature, and describe future research.
Method:
We conducted a systematic literature review (SLR) to identify properties, understand origins and evolution, and describe the emergence of community smells. This SLR explains the impact of community smells on teamwork and team performance.
Results:
We include 25 studies. Social debt describes the impacts of poor socio-technical decisions on work environments, people, software products, and society. For each of the 30 community smells identified as sources of social debt, we provide a detailed description, management approaches, organizational strategies, and mitigation effectiveness. We identify five groups of management approaches: organizational strategies, frameworks, models, tools, and guidelines. We describe 11 common properties of community smells. We develop the Community Smell Stages Framework to concisely describe the origin and evolution of community smells. We then describe the causes and effects for each community smell. We identify and describe 8 types of causes and 11 types of effects related to the community smells. Finally, we provide 8 comprehensive Sankey diagrams that offer insights into threats the community smells pose to teamwork factors and team performance.
Conclusion:
Community smells explain the influence work conditions have on software developers. The literature is scarce and focuses on a small number of community smells. Thus, the community smells still need more research. This review helps by organizing the state of the art about community smells. Our contributions provide motivations for future research and provide educational material for software engineering professionals.}
}
@article{MALHOTRA201785,
title = {On the application of search-based techniques for software engineering predictive modeling: A systematic review and future directions},
journal = {Swarm and Evolutionary Computation},
volume = {32},
pages = {85-109},
year = {2017},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2016.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2210650216303418},
author = {Ruchika Malhotra and Megha Khanna and Rajeev R. Raje},
keywords = {Search-based techniques, Change prediction, Defect prediction, Effort estimation, Maintainability prediction, Software quality},
abstract = {Software engineering predictive modeling involves construction of models, with the help of software metrics, for estimating quality attributes. Recently, the use of search-based techniques have gained importance as they help the developers and project-managers in the identification of optimal solutions for developing effective prediction models. In this paper, we perform a systematic review of 78 primary studies from January 1992 to December 2015 which analyze the predictive capability of search-based techniques for ascertaining four predominant software quality attributes, i.e., effort, defect proneness, maintainability and change proneness. The review analyses the effective use and application of search-based techniques by evaluating appropriate specifications of fitness functions, parameter settings, validation methods, accounting for their stochastic natures and the evaluation of developmental models with the use of well-known statistical tests. Furthermore, we compare the effectiveness of different models, developed using the various search-based techniques amongst themselves, and also with the prevalent machine learning techniques used in literature. Although there are very few studies which use search-based techniques for predicting maintainability and change proneness, we found that the results of the application of search-based techniques for effort estimation and defect prediction are encouraging. Hence, this comprehensive study and the associated results will provide guidelines to practitioners and researchers and will enable them to make proper choices for applying the search-based techniques to their specific situations.}
}
@article{ZEIN2023107323,
title = {Systematic reviews in mobile app software engineering: A tertiary study},
journal = {Information and Software Technology},
volume = {164},
pages = {107323},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107323},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923001787},
author = {Samer Zein and Norsaremah Salleh and John Grundy},
keywords = {Mobile app software development, Tertiary study, Systematic reviews, Mapping study},
abstract = {Context:
A number of secondary studies in the form of systematic reviews and systematic mapping studies exist in the area of mobile application software engineering.
Objective:
The focus of this paper is to provide an overview and analysis of these secondary studies of mobile app software engineering for researchers and practitioners.
Method:
We conducted a systematic tertiary study following the guidelines by Kitchenham et al. to classify and analyze secondary studies in this area.
Results:
After going through several filtration steps, we identified 24 secondary studies addressing major software engineering phases, such as initiation, requirements engineering, design, development and testing. The majority of the secondary studies focused on testing and design phases. Specific research topics addressed by the included studies were: usability evaluation, test automation, context-aware testing, cloud-based development, architectural models, effort and size estimation models, defect prediction, and GUI testing. We found that the trend in secondary studies is towards more specific areas of mobile application software engineering such as architectural design models, context-aware testing, testing of non-functional requirements, mobile cloud computing, and intelligent mobile applications. Research directions and some identified practices for practitioners were also identified.
Conclusions:
Mobile application software engineering is an active research area. The area can benefit from additional research in terms of secondary studies targeting evolution, maintenance, requirements engineering, and cross-platform mobile application development. Additionally, some of the secondary studies identify some useful practices for practitioners.}
}
@article{GONZALEZMOYANO2022107028,
title = {Uses of business process modeling in agile software development projects},
journal = {Information and Software Technology},
volume = {152},
pages = {107028},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107028},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001483},
author = {Cielo {González Moyano} and Luise Pufahl and Ingo Weber and Jan Mendling},
keywords = {Process models, Agile methodologies, Multi-method, Literature review, Thematic synthesis, Focus group},
abstract = {Context:
Agile methodologies and frameworks are widely used in software development projects because of their support for continuous change and delivery. Agile software development advocates de-prioritizing aspects such as processes and documentation. In traditional software engineering methodologies, however, business process models have been extensively used to support these aspects. Up until now, it is unclear to what extent recommendations to focus on code imply that conceptual modeling should be discontinued.
Objective:
The objective of this study is to investigate this hypothesis. More specifically, we develop a theoretical argument of how business process models are and can be used to support agile software development projects.
Method:
To this end, we use a multi-method study design. First, we conduct a systematic literature review, in which we identify studies on the usage of business process models in agile software development. Second, we apply procedures from thematic synthesis to analyze the connection between these uses and the phases of the development cycle. Third, we use a focus group design with practitioners to systematically reflect upon how these uses can help regarding four categories of challenges in agile software development: management, team, technology, and process.
Results:
From 37 relevant studies, we distill 15 different uses. The results highlight the benefits of process modeling as an instrument to support agile software development projects from different angles and in all project phases. Process modeling appears to be particularly relevant for the first phases of the development cycle, and for management and process issues in agile projects.
Conclusion:
We conclude that business process models indeed provide benefits for agile software development projects. Our findings have practical implications and emphasize the need for future research on modeling and agile development.}
}
@article{RODRIGUEZPEREZ2018164,
title = {Reproducibility and credibility in empirical software engineering: A case study based on a systematic literature review of the use of the SZZ algorithm},
journal = {Information and Software Technology},
volume = {99},
pages = {164-176},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304275},
author = {Gema Rodríguez-Pérez and Gregorio Robles and Jesús M. González-Barahona},
keywords = {Credibility, Reproducibility, SZZ Algorithm, Systematic literature review},
abstract = {Context
Reproducibility of Empirical Software Engineering (ESE) studies is an essential part for improving their credibility, as it offers the opportunity to the research community to verify, evaluate and improve their research outcomes.
Objective
We aim to study reproducibility and credibility in ESE with a case study, by investigating how they have been addressed in studies where SZZ, a widely-used algorithm by Śliwerski, Zimmermann and Zeller to detect the origin of a bug, has been applied.
Methodology
We have performed a systematic literature review to evaluate publications that use SZZ. In total, 187 papers have been analyzed for reproducibility, reporting of limitations and use of improved versions of the algorithm.
Results
We have found a situation with a lot of room for improvement in ESE as reproducibility is not commonly found; factors that undermine the credibility of results are common. We offer some lessons learned and guidelines for researchers and reviewers to address this problem.
Conclusion
Reproducibility and other related aspects that ensure a high quality scientific process should be taken more into consideration by the ESE community in order to increase the credibility of the research results.}
}
@article{DAZA2025110411,
title = {Industrial applications of artificial intelligence in software defects prediction: Systematic review, challenges, and future works},
journal = {Computers and Electrical Engineering},
volume = {124},
pages = {110411},
year = {2025},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2025.110411},
url = {https://www.sciencedirect.com/science/article/pii/S0045790625003544},
author = {Alfredo Daza and Gonzalo Apaza-Perez and Katherine Samanez-Torres and Juan Benites-Noriega and Orlando Llanos Gonzales and Pablo Cesar Condori-Cutipa},
keywords = {Machine learning, Prediction, Software defects, Metrics, Programming language},
abstract = {Software defect prediction is a constant challenge in industrial software engineering and represents a significant problem for quality and cost in software development worldwide. The purpose of this study is to gain a deeper understanding of the quartiles, countries, keywords, techniques, metrics, tools, platforms or languages, variables, data sources, and datasets used in software defect prediction. A comprehensive search of 45 articles from 2019 to 2023, using 5 databases (Scopus, ProQuest, ScienceDirect, EBSCOhost, and Web of Science), was conducted following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analysis) methodology. Results show that 60.00 % of the studies were carried out in 2023, and 68.89 % of journals were in the Q1 and Q2 quartiles. The most common techniques were Support Vector Machine (42.22 %) and Random Forest (35.56 %). The most commonly used evaluation metrics were Accuracy and F1-Score (68.89 %). Python was the main programming language (35.56 %), with Kilo (thousands) of lines of code (31.11 %) and Cyclomatic complexity (26.67 %) as key variables. Finally, NASA's Metrics Data Program Data Repository was the most used data source (31.11 %) with a dataset ranging from a minimum of 759 instances and 37 attributes to a maximum of 3579 instances and 38 attributes from 5 projects: CM1, MW1, PC1, PC3, and PC4. This systematic review provides scientific evidence on how machine learning algorithms aid in predicting software defects and improving development processes. In addition, it offers a detailed discussion by identifying trends, limitations, successful approaches, and areas for improvement, providing valuable recommendations for future research.}
}
@article{KITCHENHAM2012804,
title = {Three empirical studies on the agreement of reviewers about the quality of software engineering experiments},
journal = {Information and Software Technology},
volume = {54},
number = {8},
pages = {804-819},
year = {2012},
note = {Special Issue: Voice of the Editorial Board},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2011.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584911002321},
author = {Barbara Ann Kitchenham and Dag I.K. Sjøberg and Tore Dybå and Dietmar Pfahl and Pearl Brereton and David Budgen and Martin Höst and Per Runeson},
keywords = {Quality evaluation, Empirical studies, Human-intensive experiments, Experimentation, Software engineering},
abstract = {Context
During systematic literature reviews it is necessary to assess the quality of empirical papers. Current guidelines suggest that two researchers should independently apply a quality checklist and any disagreements must be resolved. However, there is little empirical evidence concerning the effectiveness of these guidelines.
Aims
This paper investigates the three techniques that can be used to improve the reliability (i.e. the consensus among reviewers) of quality assessments, specifically, the number of reviewers, the use of a set of evaluation criteria and consultation among reviewers. We undertook a series of studies to investigate these factors.
Method
Two studies involved four research papers and eight reviewers using a quality checklist with nine questions. The first study was based on individual assessments, the second study on joint assessments with a period of inter-rater discussion. A third more formal randomised block experiment involved 48 reviewers assessing two of the papers used previously in teams of one, two and three persons to assess the impact of discussion among teams of different size using the evaluations of the “teams” of one person as a control.
Results
For the first two studies, the inter-rater reliability was poor for individual assessments, but better for joint evaluations. However, the results of the third study contradicted the results of Study 2. Inter-rater reliability was poor for all groups but worse for teams of two or three than for individuals.
Conclusions
When performing quality assessments for systematic literature reviews, we recommend using three independent reviewers and adopting the median assessment. A quality checklist seems useful but it is difficult to ensure that the checklist is both appropriate and understood by reviewers. Furthermore, future experiments should ensure participants are given more time to understand the quality checklist and to evaluate the research papers.}
}
@article{DASILVA2012216,
title = {Towards understanding the underlying structure of motivational factors for software engineers to guide the definition of motivational programs},
journal = {Journal of Systems and Software},
volume = {85},
number = {2},
pages = {216-226},
year = {2012},
note = {Special issue with selected papers from the 23rd Brazilian Symposium on Software Engineering},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2010.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0164121210003390},
author = {Fabio Q.B. {da Silva} and A. César C. França},
keywords = {Motivation, People management, Software development, Empirical software engineering},
abstract = {Aim
In this article, factors influencing the motivation of software engineers is studied with the goal of guiding the definition of motivational programs.
Method
Using a set of 20 motivational factors compiled in a systematic literature review and a general theory of motivation, a survey questionnaire was created to evaluate the influence of these factors on individual motivation. Then, the questionnaire was applied on a semi-random sample of 176 software engineers from 20 software companies located in Recife-PE, Brazil.
Results
The survey results show the actual level of motivation for each motivator in the target population. Using principal component analysis on the values of all motivators, a five factor structure was identified and used to propose a guideline for the creation of motivational programs for software engineers.
Conclusions
The five factor structure provides an intuitive categorization for the set of variables and can be used to explain other motivational models presented in the literature. This contributes to a better understanding of motivation in software engineering.}
}
@article{NDUKWE2023111524,
title = {How have views on Software Quality differed over time? Research and practice viewpoints},
journal = {Journal of Systems and Software},
volume = {195},
pages = {111524},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111524},
url = {https://www.sciencedirect.com/science/article/pii/S016412122200200X},
author = {Ifeanyi G. Ndukwe and Sherlock A. Licorish and Amjed Tahir and Stephen G. MacDonell},
keywords = {Code snippet quality, Stack Overflow, Software quality},
abstract = {Context:
Over the years, there has been debate about what constitutes software quality and how it should be measured. This controversy has caused uncertainty across the software engineering community, affecting levels of commitment to the many potential determinants of quality among developers. An up-to-date catalogue of software quality views could provide developers with contemporary guidelines and templates. In fact, it is necessary to learn about views on the quality of code on frequently used online collaboration platforms (e.g., Stack Overflow), given that the quality of code snippets can affect the quality of software products developed. If quality models are unsuitable for aiding developers because they lack relevance, developers will hold relaxed or inappropriate views of software quality, thereby lacking awareness and commitment to such practices.
Objective:
We aim to explore differences in interest in quality characteristics across research and practice. We also seek to identify quality characteristics practitioners consider important when judging code snippet quality. First, we examine the literature for quality characteristics used frequently for judging software quality, followed by the quality characteristics commonly used by researchers to study code snippet quality. Finally, we investigate quality characteristics used by practitioners to judge the quality of code snippets.
Methods:
We conducted two systematic literature reviews followed by semi-structured interviews of 50 practitioners to address this gap.
Results:
The outcomes of the semi-structured interviews revealed that most practitioners judged the quality of code snippets using five quality dimensions: Functionality, Readability, Efficiency, Security and Reliability. However, other dimensions were also considered (i.e., Reusability, Maintainability, Usability, Compatibility and Completeness). This outcome differed from how the researchers judged code snippet quality.
Conclusion:
Practitioners today mainly rely on code snippets from online code resources, and specific models or quality characteristics are emphasised based on their need to address distinct concerns (e.g., mobile vs web vs standalone applications, regular vs machine learning applications, or open vs closed source applications). Consequently, software quality models should be adapted for the domain of consideration and not seen as one-size-fits-all. This study will lead to targeted support for various clusters of the software development community.}
}
@article{WANG2023107327,
title = {Application of knowledge graph in software engineering field: A systematic literature review},
journal = {Information and Software Technology},
volume = {164},
pages = {107327},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107327},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923001829},
author = {Lu Wang and Chenhan Sun and Chongyang Zhang and Weikun Nie and Kaiyuan Huang},
keywords = {Knowledge graph, Software engineering, Code recommendation, API recommendation, Vulnerability mining and location, Intelligent development},
abstract = {Context:
Knowledge graphs describe knowledge resources and their carriers through visualization. Moreover, they mine, analyze, construct, draw, and display knowledge and their interrelationships to reveal the dynamic development law of the knowledge field. Furthermore, knowledge graphs provide practical and valuable references for subject research. With the development of software engineering, powerful semantic processing and organizational interconnection capabilities of knowledge graphs are gradually required. Current research suggests using knowledge graphs for code or API recommendation, vulnerability mining, and positioning to improve the efficiency and accuracy of development and design. However, software engineering lacks a systematic analysis of the knowledge graphs application.
Objective:
This paper explores the construction techniques and application status of knowledge graphs in the field of software engineering, broadens the application prospects of knowledge graphs in this field, and facilitates the subsequent research of researchers.
Methods:
We collected over 100 documents from 2017 to date and selected 55 directly related documents for systematic analysis. Then, we analyzed the organized knowledge mainly stored in software engineering knowledge graphs, including software architecture, code details, and security reports.
Results:
We studied the emerging research methods in ontology modeling, named entity recognition, and knowledge fusion in graph construction and found that current knowledge graphs are mainly used in intelligent software development, software vulnerability mining, security testing, and API recommendation.
Conclusion:
Our research on the innovation of knowledge graph in software engineering and the future construction of integrating open-source community software and developer recommendations with knowledge-driven microservice O&M aspects can inspire more scholars and knowledge workers to use knowledge graph technology, which is important to solve software engineering problems and promote the development of both fields.}
}
@article{MIZUTANI2021100421,
title = {Software architecture for digital game mechanics: A systematic literature review},
journal = {Entertainment Computing},
volume = {38},
pages = {100421},
year = {2021},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2021.100421},
url = {https://www.sciencedirect.com/science/article/pii/S1875952121000185},
author = {Wilson K. Mizutani and Vinícius {K. Daros} and Fabio Kon},
keywords = {Systematic literature review, Digital games, Software architecture},
abstract = {Game mechanics, the rules that simulate the virtual world inside a game, take a great part in what makes a game unique. For digital games, this uniqueness reduces the opportunity for software reuse. A high-level software architecture for game mechanics, however, can still be reused where a single, specific implementation cannot. Despite that potential, existing research on game development lacks a comprehensive analysis of how game mechanics could benefit from the field of software architecture. This limits the opportunities for developers and researchers alike to benefit from findings on the subject. To help guide future research on game development, we analyzed the state-of-the-art architectures in game mechanics through a systematic literature review. This work carefully documents data from 36 studies, analyzing the reflections and compromises between design requirements, practices, and restrictions, as well as how they contribute to different types of mechanics. The main findings are that researchers favor reduced development complexity, but often tailor their solutions to specific games or genres. We conclude that a valuable avenue for future research in the field is the generalization of architectural solutions around specific types of mechanics and formalizing the use of software engineering for game mechanics.}
}
@article{ALNASEEF2023107315,
title = {Towards a successful secure software acquisition},
journal = {Information and Software Technology},
volume = {164},
pages = {107315},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2023.107315},
url = {https://www.sciencedirect.com/science/article/pii/S0950584923001702},
author = {Faisal Alnaseef and Mahmood Niazi and Sajjad Mahmood and Mohammad Alshayeb and Irfan Ahmad},
keywords = {Systematic reviews, Empirical software engineering, Software security, Software acquisition, Software process},
abstract = {Context
Security is a critical attribute of software quality. Organizations invest considerable sums of money in protecting their assets. Despite investing in secure infrastructure, organizations remain prone to security risks and cyberattacks that exploit security flaws. Many factors contribute to the challenges related to software security, e.g., the exponential increase in Internet-enabled applications, threats from hackers, and the susceptibility of inexperienced Internet users. Moreover, organizations tend to procure off-the-shelf software from third-party suppliers. However, gaining a complete understanding of ways to assess suppliers’ readiness to provide secure software before selecting a supplier is imperative.
Objective
We have developed a readiness model for secure software acquisition (RMSSA) to help software organizations select suppliers who can provide secure software.
Method
We employed state-of-the-art techniques based on systematic literature review to determine the best practices undertaken by organizations in terms of acquiring secure software, which depends on six core security knowledge areas: confidentiality, integrity, availability, authorization, authentication, and accountability.
Results
We evaluated the RMSSA theoretically and in a practical environment based on three case studies with software organizations. Our findings can guide software organizations in selecting the supplier who can develop secure software.
Conclusion
The proposed RMSSA can be used to evaluate suppliers’ readiness to provide secure software.}
}
@article{PONCE2025107870,
title = {Microservices testing: A systematic literature review},
journal = {Information and Software Technology},
volume = {188},
pages = {107870},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107870},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925002095},
author = {Francisco Ponce and Roberto Verdecchia and Breno Miranda and Jacopo Soldani},
keywords = {Microservices, Microservice architecture, Software testing, Functional testing, Non-functional testing, Testing strategies, Systematic literature review},
abstract = {Context:
Microservices offer scalability and resilience for modern cloud-native applications but present significant challenges in software testing due to their distributed and heterogeneous nature.
Objective:
This study aims to consolidate and classify the current body of knowledge on microservice testing through a systematic literature review, providing actionable insights for both researchers and practitioners.
Methods:
Following established guidelines for systematic literature reviews in software engineering, we identified 74 primary studies relevant to microservices testing. These studies were systematically categorized using the SWEBOK (Software Engineering Body of Knowledge) taxonomy for software testing. Specifically, we classified the identified techniques according to their testing objectives, levels, strategies, and types. We also evaluated the study types to gauge the maturity and readiness of the current state-of-the-art in microservice testing.
Results:
System testing emerged as the most frequently investigated testing level, followed by integration, unit, and acceptance testing. Conformance, regression, and API testing were the most common functional testing objectives, while performance efficiency and reliability were instead predominant in the case of non-functional testing. Specification-based testing strategies were the most adopted, followed by usage-based and fault-based ones. Additionally, most studies employed laboratory experiments and had low-to-medium technology readiness levels, indicating early-stage maturity. The systems under test varied in size and domain, with TrainTicket being the most widely used reference benchmark for large systems.
Conclusion:
While significant progress has been made in microservice testing, the field remains fragmented, with notable gaps in areas such as, e.g., flexibility and security testing. The dominance of early-stage proposals highlights the need for more empirical validation and industry-grade benchmarks to facilitate broader adoption. This review offers a structured roadmap for future research and practical adoption in microservices testing.}
}